\documentclass[review]{elsarticle}
\usepackage{amsfonts}
\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{amsmath, amssymb , graphicx }
\usepackage{algorithm} 
\usepackage[noend]{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{positioning}

\newenvironment{proof}[1][Proof.]{ \begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{booktabs}

\usepackage{isomath}
\usepackage{latexsym,    amscd, amsfonts, enumerate, color}
\journal{Computational and Applied Mathematics}

\newtheorem{class}{Class   }
\newtheorem{theorem}{Theorem }
\newtheorem{remark}{Remark  }
\newtheorem{example}{Example  }
\newtheorem{corollary}{Corollary  }
\newtheorem{definition}{Definition  }
\newtheorem{hypothesis}{Hypothesis H1  }
\newtheorem{problem}{Problem }

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\setlength{\textwidth}{160mm}
\setlength{\textheight}{250mm}
\setlength{\topmargin}{-20mm}
\setlength\oddsidemargin   {1\p@}
\setlength\evensidemargin  {7\p@}
\setlength\marginparwidth{0\p@}
\setlength\headsep   {0\p@}
\newdimen\bibindent
\setlength\bibindent{\parindent}
\setlength{\parskip}{\z@ \@plus \p@}
\setlength{\hfuzz}{2\p@}
\setlength{\arraycolsep}{1.5\p@}
\tolerance=500
\predisplaypenalty=0
\clubpenalty=10000
\widowpenalty=10000
\setlength\footnotesep{7.7\p@}
\newdimen\betweenumberspace    % dimension for space between
\betweenumberspace=5\p@         % number and text of titles
\newdimen\headlineindent            % dimension for space of
\headlineindent=2.5cc                  % number and gap of running
\setlength{\footskip}{15mm}
 

\makeatother


%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}

\begin{document}

\begin{frontmatter}

%\title{An Efficient Approach for Solving Nonlinear Volterra Integral Equations via hybrid Kernel Methods}
%روشی کارآمد برای حل معادلات انتگرال غیرخطی ولترا با استفاده از روش‌های هسته ترکیبی
%\title{A Novel Hybrid Kernel Technique for Nonlinear Volterra Integral Equations}
%روشی ابتکاری مبتنی بر هسته‌های ترکیبی برای معادلات انتگرال ولترای غیرخطی
%\title{Breaking Nonlinearity: Advanced Hybrid Kernel Techniques for Volterra-Type Integral Equations}
%شکستن غیرخطی‌گرایی: تکنیک‌های پیشرفته هسته ترکیبی برای معادلات انتگرالی نوع ولترا
%\title{Exact Solvability and Convergence Analysis of Nonlinear Volterra Equations with Hybrid Kernels}
%بررسی حل‌پذیری دقیق و تحلیل همگرایی روش‌های هسته هیبریدی برای معادلات انتگرالی ولترای غیرخطی
%\title{Stable and Accurate Solutions of Volterra Integral Equations via Optimized Hybrid Radial Kernels: A Meshless Computational Approach}
%حل پایدار و دقیق معادلات انتگرال ولترا با هسته‌های شعاعی هیبریدی بهینه: یک روش بدون شبکه
\title{Optimized Hybrid Radial Kernels for Stable and Accurate Solutions of Volterra Integral Equations: A Meshless Approach}
\author[a] {Tahereh Akbari}
\ead{MrsTahere\_Akbari@yahoo.com}

\author[a] {Mohsen Esmaeilbeigi \corref{cor1}}
\ead{m.esmaeilbeigi@malayeru.ac.ir}

\author[a] {Davoud Moazami}
\ead{davoud.moazami@stu.malayeru.ac.ir}





 \cortext[cor1]{Corresponding author}
%% Group authors per affiliation:
\address[a]{Faculty of Mathematical Sciences and Statistics, Malayer University, Malayer, Iran}

%% or include affiliations in footnotes:
 
\begin{abstract}
%A central issue in kernel-based approximation theory and its practical applications is the trade-off between accuracy and numerical stability. One straightforward yet powerful solution to this problem is the use of hybrid kernels. In this paper, we propose a meshless computational scheme employing hybrid radial kernels (HRKs) to solve the Volterra integral equations (VEIs). The approach approximates the solution through a discrete collocation method, utilizing a hybrid kernel constructed by combining suitable base kernels with optimized weight parameters. The optimal weight parameters for hybrid kernels are determined using the particle swarm optimization (PSO) algorithm, minimizing the root-mean-square (RMS) error. In addition, we analyze the convergence properties of the proposed hybrid method. To validate its effectiveness, we present numerical experiments that compare accuracy and stability. The results confirm that the hybrid technique significantly mitigates the ill-conditioning of operational matrices while maintaining high precision and stability across varying shape parameter values.

%The duality of accuracy versus stability in kernel-based methods presents a fundamental challenge in computational mathematics, particularly for Volterra integral equations (VIEs). To address this, we introduce a meshless computational scheme leveraging optimized hybrid radial kernels (HRKs), which strategically combine base kernels via weight parameters tuned using Particle Swarm Optimization (PSO). This hybridization not only mitigates the ill-conditioning inherent in traditional kernel methods but also preserves spectral accuracy—a critical advancement for problems with singular or oscillatory solutions.
%Our framework employs a discrete collocation method with HRKs, where the PSO-driven optimization minimizes the RMS error to ensure optimal kernel blending. We further provide a rigorous convergence analysis, demonstrating that the hybrid kernel’s adaptive structure extends the solvable regime of VIEs beyond classical limitations. Numerical experiments validate the method’s superiority, showing enhanced stability across shape parameters and outperforming single-kernel approaches in both smooth and non-smooth problem classes. This work bridges theoretical kernel design with practical computation, offering a scalable tool for nonlinear integral equations.
A central challenge in kernel-based approximation theory is balancing accuracy with numerical stability. Hybrid kernels provide an effective solution to this trade-off. In this paper, we present a meshless scheme utilizing hybrid radial kernels (HRKs) to solve Volterra integral equations (VIEs). The solution is approximated using a discrete collocation method, where the HRK is formed by combining suitable base kernels with optimized weight parameters. These parameters are determined via the particle swarm optimization (PSO) algorithm, which minimizes the root-mean-square (RMS) error. We also provide a convergence analysis to confirm the method’s theoretical reliability. Numerical experiments validate the approach, demonstrating its accuracy and stability compared to existing methods. The results show that this hybrid technique effectively reduces ill-conditioning in operational matrices while preserving high precision and stability across a range of shape parameter values.
\end{abstract}

\begin{keyword}
   Volterra integral equations;  Hybrid kernels; Particle swarm optimization;   Accuracy;   Stability.
% \PACS{PACS code1 \and PACS code2 \and more}
 
\end{keyword}

\end{frontmatter}


\section{Introduction}\label{sec1}
%Consider $d$-dimensional nonlinear Volterra integral equations of the second kind, namely
%Volterra integral equations (VIEs) are a class of integral equations where the limits of integration are variable, typically involving an upper limit that depends on the independent variable. Unlike Fredholm equations, VIEs often exhibit unique solutions under mild conditions. They are closely related to initial-value problems in differential equations and are frequently solved using numerical or iterative methods \citep{a00}.
%This study considers nonlinear Volterra integral equations (VEIs) in a $d$-dimensional space, formulated as follows:
%
%
%
%\begin{equation}\label{equ1}
%u(\textbf{x})-\lambda\int_{a_1}^{x_1}\dots\int_{a_d}^{x_d} K(\textbf{x},\textbf{y},u(\textbf{y)}) dy_1 \dots dy_d=f(\textbf{x}),\quad \textbf{x}=(x_1,\dots ,x_d), \textbf{y}=(y_1,\dots ,y_d)\in\Omega,
%\end{equation}
%where the domain $\Omega\subset\mathbb{R}^d$ is a rectangular region defined by $[a_1,b_1]\times\dots\times[a_d,b_d]$. The function $K$ is a continuously differentiable kernel over $\Omega\times\Omega\times\mathbb{R}$, and $f$ is a continuous function within $\Omega$. The goal is to   determine the unknown function $u(\textbf{x})$, with $\lambda$ as a nonzero constant.  
%         
%The theory of VIEs originated from Vito Volterra’s work on hereditary phenomena (Volterra, 1896), later formalized in his functional analysis studies (Volterra, 1959). Its distinction from Fredholm-type equations is discussed in modern treatments (e.g., Hackbusch, 1995) \cite{a001}. VIEs play a crucial role in modeling numerous scientific and engineering phenomena. These equations are particularly valuable for analyzing heat transfer processes in various materials and systems \cite{a1}. In mechanical and physical systems \cite{a2}, they provide powerful tools for solving real-world problems involving complex interactions. The equations have proven especially useful in studying unsteady viscous flows through confined geometries like pipes and channels \cite{a3}. Researchers frequently employ them to model diffusion mechanisms across different media and conditions \cite{a4}. In the field                                         of materials science, VIEs help characterize the behavior of electroelastic materials under external stimuli \cite{a5}. They offer robust solutions for contact mechanics problems involving interacting surfaces \cite{a6}. Plasma physics researchers utilize these equations to describe various plasma phenomena and interactions \cite{a7}. Image processing applications include advanced deblurring algorithms and regularization techniques for image enhancement \cite{a8}.
% The equations also facilitate analysis of axisymmetric contact scenarios in materials with intricate rheological properties. In wave physics, they contribute to diffraction theory developments \cite{a9}. Electrochemical systems benefit from their application, particularly in modeling microband electrode behavior when diffusion coefficients are equal \cite{a10}. Their versatility makes them indispensable across these diverse disciplines, enabling researchers to tackle complex, nonlinear problems that arise in advanced scientific investigations and engineering applications.  
%Over the years, various numerical approaches have been developed to solve Volterra integral equations, including collocation methods \cite{a11}, \cite{a12}, \cite{a13} , Galerkin techniques \cite{a14}, \cite{a15} , Bernstein approximations \cite{a16} , Chebyshev operational vectors \cite{a17} , wavelet-based schemes \cite{a18, a19, a20, a21} , Jacobi-collocation approaches \cite{a22} , and reproducing kernel Hilbert space methods \cite{a23}. For singular Abel-type integral equations, generalized quadrature techniques have been employed \cite{a24}. Additionally, a novel numerical algorithm combining geometric series theorems with Schauder bases has been introduced for solving certain two-dimensional Volterra integral equations \cite{a25}. Furthermore, iterative approaches like the discrete Galerkin \cite{a26} and collocation methods \cite{a27}, as well as spline-based Galerkin schemes \cite{a28}. Other methods involve Nyström techniques \cite{a29}, rationalized Haar functions in two dimensions \cite{a30}, and differential transform approaches \cite{a31} have been applied to approximate solutions for two-dimensional Volterra integral equations. Additionally, researchers have employed degenerate kernel methods, fast collocation algorithms, piecewise polynomial projections, and Gauss product quadrature rules to address these equations.
%
%Radial basis functions (RBFs) have emerged as powerful tools for scattered data interpolation, demonstrating remarkable effectiveness in approximating unknown functions across various applications. Their formulation depends on a single variable irrespective of problem dimensionality, making them particularly advantageous for high-dimensional problems. Notably, RBF-based approaches eliminate the need for domain discretization, offering significant computational benefits. The foundation of RBF theory was established by Hardy \cite{a32} through his pioneering work on multiquadric and inverse multiquadric functions for modeling geophysical phenomena. Subsequent developments included Meinguet's \cite{a33} thin plate splines (a parameter-free RBF variant) and Wahba's \cite{a34} applications for multidimensional data smoothing. Franke's comprehensive review \cite{a35} systematically evaluated various scattered data approximation techniques prevalent in the early 1980s.   
%The application spectrum of RBFs has evolved considerably, transitioning from pure interpolation to becoming a robust framework for partial differential equation (PDE) solutions. Kansa's groundbreaking 1990 work \cite{a36} introduced RBF-based collocation for PDEs, spawning numerous applications including:
%
%\begin{itemize}
%    \item Shock wave modeling in 1D nonlinear Burgers' equations \cite{a37}
%     \item Hydrodynamic simulations of shallow water systems \cite{a38}
%      \item Thermal analysis in heat transfer problems \cite{a39}
%       \item Nonlocal boundary value problems \cite{a040} 
%          \item Wave dynamics in improved Boussinesq models \cite{a041}
%\end{itemize}
%
%Modern computational mathematics is witnessing a paradigm shift through the advancement of meshless methods, which fundamentally challenge traditional grid-dependent approaches \cite{a40}. By eschewing conventional domain discretization, these innovative techniques employ sophisticated mathematical tools like radial basis functions \cite{a41} and moving least squares approximations to construct solutions directly from scattered nodes. The inherent flexibility of meshless formulations makes them particularly powerful for handling problems with complex geometries, moving boundaries \cite{a42}, or higher dimensions where classical mesh-based methods struggle. Their adaptive nature allows for dynamic refinement \cite{a43} in critical regions without the topological constraints of elements, enabling more efficient simulation of phenomena like crack propagation or multiphase flows \cite{a44}.
%
% Recent developments have demonstrated remarkable success in diverse applications ranging from nanoscale molecular interactions to large-scale geophysical modeling \cite{a45}, consistently outperforming traditional methods in accuracy while reducing preprocessing overhead \cite{a46}. As computational demands grow increasingly complex, meshless techniques are rapidly emerging as the framework of choice for next-generation numerical analysis across engineering and scientific disciplines. These methods offer significant benefits, including simplicity of implementation, geometric flexibility, and the potential for spectral (or exponential) convergence rates when using infinitely smooth radial basis functions (RBFs). The highest accuracy is typically achieved with a small radial kernel shape parameter, meaning the RBF is nearly flat. However, a major drawback arises when applying conventional computation techniques with infinitely smooth kernels: as the shape parameter approaches zero, the standard operational matrix becomes highly ill-conditioned, creating a trade-off between precision and numerical stability. Thus, the primary challenge in employing these techniques lies in mitigating this ill-conditioning while maintaining the method's accuracy. The relationship between kernel smoothness and computational ill-conditioning has long been explored in the literature. Various numerical approaches have been developed to address this issue when using radial kernels. An early stabilization method for multiquadric kernels in the flat limit overcame the real shape parameter restriction through the Contour-Padé technique \cite{a47}, though it was limited to small node sets. Later, the RBF-QR method combined QR decomposition \cite{a48}, \cite{a49} with kernel series expansions for stable interpolation with zonal and Gaussian kernels. Another stable approach leveraged Mercer’s theorem to derive a new basis for Gaussian interpolation from the kernel’s eigenfunction expansion in multivariate space \cite{a50}.
% 
%Hybrid kernels offer an alternative approach to mitigate ill-conditioning issues. First presented in \cite{a51}, the Gaussian-cubic hybrid kernel enhances interpolation stability by blending cubic elements into Gaussian bases. This strategic combination was proven to optimize conditioning parameters, ensuring algorithmic robustness for scattered datasets. Recent advancements in hybrid kernel methodologies have demonstrated remarkable versatility across computational applications. Research by \cite{a52} established that these kernels enable stable PDE solutions through modified RBF-PS approaches, particularly effective when handling systems with numerous degrees of freedom. The technique's adaptability was further evidenced in \cite{a53}, where it successfully reconstructed temperature fields from experimental measurement data. In neurodynamics, \cite{a54} developed a specialized local implementation for solving fractional cable equations, while \cite{a55} applied the methodology to analyze viscous flows by solving Burgers' equation across varying Reynolds number conditions. These collective findings underscore the method's broad applicability in scientific computing.
%This hybridization approach strategically combines complementary strengths of distinct kernels while mitigating their individual limitations, all within a standard RBF framework. Key features distinguish this methodology: First, its meshfree nature enables flexible application across arbitrary domains regardless of dimensionality. Second, the strategic fusion of smooth and non-smooth kernel components yields dual benefits of precision enhancement and conditioning improvement. Third, the technique admits natural extension to broader integral equation classes. Finally, it offers practical computational advantages through simple implementation \cite{a56}.

%Volterra integral equations (VIEs) are a class of integral equations characterized by variable upper limits of integration, distinguishing them from Fredholm-type equations \citep{a00}. These equations frequently arise in modeling hereditary phenomena and initial-value problems, with applications spanning heat transfer \citep{a1}, unsteady viscous flows in confined geometries \citep{a3}, electroelastic material behavior \citep{a5}, contact mechanics \citep{a6}, plasma physics \citep{a7}, image processing algorithms \citep{a8}, and electrochemical systems \citep{a10}. The study focuses on nonlinear VIEs in $d$-dimensional space, formulated with a continuously differentiable kernel $K$ and continuous function $f$ over a rectangular domain $\Omega$. The goal is to determine the unknown function $u(\mathbf{x})$ given a nonzero constant $\lambda$:
%
%\begin{equation}\label{equ1}
%u(\mathbf{x})-\lambda\int_{a_1}^{x_1}\dots\int_{a_d}^{x_d} K(\mathbf{x},\mathbf{y},u(\mathbf{y})) dy_1 \dots dy_d=f(\mathbf{x})
%\end{equation}
%
%The theoretical foundations of VIEs trace back to Vito Volterra's pioneering work \citep{a001}, with modern applications demonstrating their versatility across scientific and engineering disciplines.
%
%Numerical approaches for solving VIEs have evolved significantly, encompassing collocation methods \citep{a11,a12,a13}, Galerkin techniques \citep{a14,a15}, Bernstein approximations \citep{a16}, Chebyshev operational vectors \citep{a17}, wavelet-based schemes \citep{a18,a19,a20,a21}, Jacobi-collocation methods \citep{a22}, and reproducing kernel Hilbert space methods \citep{a23}. Recent advances include specialized algorithms for singular Abel-type equations \citep{a24} and innovative combinations of geometric series theorems with Schauder bases for two-dimensional cases \citep{a25}.
%
%Radial basis functions (RBFs) have emerged as particularly powerful tools for scattered data interpolation and VIE solutions, owing to their dimensionality-independent formulation \citep{a32}. The RBF framework, originating from Hardy's multiquadric functions \citep{a32}, has expanded to include thin plate splines \citep{a33} and diverse applications in partial differential equation (PDE) solutions \citep{a36}, ranging from shock wave modeling \citep{a37} to thermal analysis \citep{a39} and hydrodynamic simulations \citep{a38}.
%
%Modern computational mathematics is witnessing a paradigm shift through meshless methods \citep{a40,a41}, which offer advantages for problems with complex geometries or moving boundaries \citep{a42}. These methods demonstrate remarkable success in applications from nanoscale interactions to geophysical modeling \citep{a45}, often outperforming traditional methods in accuracy while reducing preprocessing overhead \citep{a46}.
%
%A critical challenge in RBF methods involves the trade-off between accuracy and numerical stability as the shape parameter approaches zero. Various techniques address this issue, including the Contour-Padé technique \citep{a47}, RBF-QR method \citep{a48,a49}, and eigenfunction expansions \citep{a50}. Hybrid kernel approaches \citep{a51} that combine smooth and non-smooth components, such as Gaussian-cubic hybrids, have shown particular promise in optimizing conditioning while maintaining precision.
%
%These hybrid methods have demonstrated success in diverse applications including temperature field reconstruction \citep{a53}, fractional cable equations in neurodynamics \citep{a54}, and viscous flow analysis \citep{a55}. The current work introduces three key innovations: a parameter-optimized collocation scheme using hybrid kernels, formal convergence analysis, and numerical validation demonstrating shape-parameter-independent stability. The methodology's meshfree nature and improved conditioning position it as a significant advancement over conventional techniques for solving nonlinear VIEs \citep{a56}.
%
%Building on these foundations, this work introduces hybrid bases to nonlinear VIE resolution through three key innovations: First, a parameter-optimized collocation scheme blending kernel functions. Second, formal convergence analysis. Third, comprehensive numerical validation demonstrating shape-parameter-independent stability and accuracy a significant advancement over conventional methods.
%The paper progresses through six technical components: Beginning with theoretical preliminaries of HRKs (\ref{sec2}), we then develop their meshless implementation for VIE solutions (\ref{sec3}). Analytical convergence results follow in Section \ref{sec4}, preceding our novel parameter selection methodology (\ref{sec5}). Comprehensive numerical verification across dimensional cases appears in Section \ref{sec6}, with final conclusions in Section \ref{sec7}.
%



VIEs are a class of integral equations distinguished by their variable upper limits of integration, setting them apart from Fredholm-type equations \citep{a00}. {\color{red} VIEs have many applications in various areas such as:  
\begin{itemize}
\item[$\bullet$] The following nonlinear Volterra integral equation (VIE) arises in the analysis of the neural networks with a post-inhibitory rebound \citep{a11}, 
\[
u(x)=1+ \int_{0}^{x} (x-t)^3  (4-x+t) e^{t-x} \frac{u^4(t)}{1+2u^2(t)+2u^4(t)} dt, \quad x \in [0,10].
\]

\item[$\bullet$] The following VIE arises in the analysis of the reflexion of sound pulses \citep{aa12}, 
\[
u(x)=f(x)- \int_{0}^{x}  \frac{2}{(x-t+2)^2}g(u(t)) dt, \quad x \in [0,40].
\]


\item[$\bullet$] Some nonlinear VIEs arise as a reformulation of initial value problems which occur in many
problems of mathematical physics. For example, consider the Lane-Emden type equations arise in the analysis of
the gravitational potential of the degenerate white-dwarf stars, the isothermal gas sphere, the static stellar models
in Newtonian gravity and other problems in the mathematical physics \citep{aa12}, which are defined as follows: 
\[
u^{''}+\frac{2}{s}u^{'}+g(s)f(u)=h(s), \quad 0<s< \infty,
\]
subject to 
\[
u(0)=a, \quad u^{'}(0)=0, 
\]
where $a$ is a constant and $f,g$ and $h$ are determined functions. The given Lane-Emden problem can be transformed
into the following VIE \citep{aa12, bb1}
\[
u(x)=a+\int_{0}^{x} \Bigl ( \frac{t^2}{x}-t \Bigr ) (g(u) f(u(t)-h(t))) dt, \quad x \in (0,\infty).
\]


\end{itemize}
}
In this article, we focus on nonlinear VIEs in $d$-dimensional space, expressed as:
\begin{equation}\label{equ1}
u(\mathbf{x}) - \lambda \int_{a_1}^{x_1} \cdots \int_{a_d}^{x_d} K(\mathbf{x}, \mathbf{y}, u(\mathbf{y})) \, d\mathbf{y} = f(\mathbf{x}),
\end{equation}
where $\mathbf{x} \in \Omega \subset \mathbb{R}^d$, $K$ is a continuously differentiable kernel, $f$ is a continuous function, and $\lambda \neq 0$. Our objective is to determine the unknown function $u(\mathbf{x})$.

Numerical methods for solving VIEs have advanced significantly over time. Traditional approaches include collocation methods \citep{a11, a13}, Galerkin techniques \citep{a14,a15}, and spectral methods such as those using Chebyshev polynomials \citep{a17} or Jacobi polynomials \citep{a22}. More recent developments encompass wavelet-based schemes \citep{a18,a19,a20,a21}, reproducing kernel Hilbert space methods \citep{a23}, and specialized algorithms for singular \citep{a24} or multi-dimensional VIEs \citep{a25}. These methods provide a robust foundation for tackling the computational challenges posed by VIEs.

Among modern tools, meshless methods based on radial basis functions (RBFs) and radial kernels stand out as cutting-edge solutions. RBFs, first introduced with Hardy's multiquadrics \citep{a32} and later expanded to include forms like thin plate splines \citep{a33}, excel in scattered data interpolation and are particularly effective for VIEs due to their dimension-independent formulation \citep{a32}. Meshless methods that leverage RBF offer significant advantages, especially for problems involving complex geometries or moving boundaries \citep{a42}. Their applications span partial differential equation (PDE) solutions in fluid dynamics \citep{a38}, and thermal analysis \citep{a39},  often outperforming traditional methods in accuracy and computational efficiency \citep{a46}. {\color{red} Also, the RBFs have been applied for the numerical solution of VIEs  \citep{assari1}. 
Furthermore, a meshless local Galerkin method was introduced in \citep{assari2} as an effective means to solve VIEs  derived from nonlinear fractional differential equations.}

However, a key challenge in RBF-based methods is the trade-off between accuracy and numerical stability. As the shape parameter in RBFs decreases, interpolation accuracy increases, but the system matrix's condition number worsens, leading to numerical instability. {\color{red} To mitigate this, a range of modern strategies has been developed, including curvature-adaptive schemes \citep{a47}, data-driven approaches leveraging neural networks or regression models \citep{a48}, and Bayesian optimization techniques \citep{a49}, which systematically tune the shape parameter to balance precision and stability. Additionally, the Hilbert–Schmidt SVD (HS-SVD) approach provides a stable mechanism for replacing a set of near-flat kernels with scattered centres to a well-conditioned basis for exactly the same space \citep{a50, a480, a481}. These methods aim to balance  accuracy and numerical stability more systematically than traditional trial and-error approaches. }

Another stable  approach to addressing ill-conditioning is the use of hybrid kernels \citep{a51}, which provide an effective and attractive solution owing to their conceptual simplicity and ease of implementation. By combining smooth and non-smooth RBFs such as Gaussian and cubic kernels hybrid kernel methods improve the system's conditioning while maintaining high approximation accuracy. These methods have proven successful in applications such as temperature field reconstruction \citep{a53}, neurodynamic modeling \citep{a54}, and viscous flow analysis \citep{a55}. In this article, we employ the hybrid kernel technique to address the accuracy-stability trade-off in the numerical solution of nonlinear VIEs, leveraging its straightforward implementation and robust performance.

Our work introduces a novel approach to solving nonlinear VIEs in $d$-dimensions using hybrid kernels, focusing on three main contributions: a parameter-optimized collocation scheme, a formal convergence analysis, and comprehensive numerical validation demonstrating stability and accuracy across a range of shape parameters. This method enhances the meshless advantages of RBFs while overcoming limitations of traditional techniques \citep{a56}. The article is structured as follows:

\begin{itemize}
    \item \textbf{Section \ref{sec2}}: Theoretical preliminaries of hybrid kernels.
    \item \textbf{Section \ref{sec3}}: Meshless implementation for solving VIEs.
    \item \textbf{Section \ref{sec4}}: Analytical convergence results.
    \item \textbf{Section \ref{sec5}}: Novel parameter selection methodology.
    \item \textbf{Section \ref{sec6}}: Numerical verification across dimensional cases.
    \item \textbf{Section \ref{sec7}}: Conclusions and future directions.
\end{itemize}

This structure guides readers through the theoretical foundations, practical implementation, and empirical validation of our approach, highlighting the hybrid kernel technique as a significant advancement in VIE solvers.


%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%\section{Preliminaries }\label{sec2}
%This section covers essential definitions and mathematical foundations of hybrid kernels used in this study.
%\subsection{Approximation by HRKs}
%%First, hybrid radial kernels (HRKs) can approximate a function in any dimension.
%At first,  the HRKs are used to approximate a function in any dimensions.
%\begin{definition}\cite{44}\label{deff1}
%A kernel $\Phi:\mathbb{R}^d\longrightarrow\mathbb{R}$ is termed radial when a univariate function exists $ \phi:[0,\infty)\longrightarrow\mathbb{R} $ such that 
%\[\Phi(\mathbfit{x})=\phi(r),\]
%where $r=\Vert \mathbfit{x}\Vert$ and $\Vert . \Vert$ is some norm on $\mathbb{R}^d$, usually the Euclidean norm.
%\end{definition}
%Let $\mathit{X}=\{ \mathbfit{x}_i\}_{i=1}^{n}$ be a set of dispersed nodes  on the region $\Omega\subset\mathbb{R}^d$. To estimate   a function $u(\mathbfit{x})$ at a point $\mathbfit{x}$ within the region  $\Omega$ using the kernel $\Phi(\mathbfit{x})$ based on the points set $\mathit{X}$, the following linear combination is considered: 
%\begin{equation}\label{eq10}
%u(\mathbfit{x})\approx\mathcal{P}_n u(\mathbfit{x})=\sum_{j=1}^n\alpha_j\phi(\Vert \mathbfit{x}-\mathbfit{x}_j\Vert),\quad x\in \Omega.
%\end{equation}
%The coefficients $\mathbfit{\alpha}=[\alpha_1,\alpha_2,...,\alpha_n]^T$ are derived by fulfilling the interpolation conditions 
%\begin{equation} 
%\mathcal{P}_n u(\mathbfit{x}_i)=u(\mathbfit{x}_i) \quad i=1,2, \cdots, n,
%\end{equation}
%i.e. they can be obtained by solving a linear system 
%\begin{equation}\label{eq11}
%\mathbf{A}\mathbfit{ \alpha=}\mathbf{u},
%\end{equation}
%in which  $\mathbf{A}_{jk}=[\phi(\Vert \mathbfit{x}_i-\mathbfit{x}_j\Vert)]_{i,j=1}^n$ and $\mathbf{u}=[u(\mathbfit{x}_1),u(\mathbfit{x}_2),...,u(\mathbfit{x}_n)]^T$. Radial kernels can be classified into two main categories: infinitely smooth and piecewise smooth kernels, as illustrated in Table \ref{tab1}. Among the infinitely smooth kernels, Gaussian (GA) and inverse multiquadrics (IMQ) generate positive definite coefficient matrices A in (\ref{eq11}), while multiquadrics (MQ) produce matrices with a single positive eigenvalue and all others negative, guaranteeing invertibility \cite{22, 31, 37, 39, 40, 42}. On the other hand, the piecewise smooth kernels listed in Table \ref{tab1} exhibit symmetry and conditional strict positive definiteness.
%Consequently, employing lower-degree polynomials is often required to ensure invertibility:
%\[ \Lambda(\mathbfit{x})=\sum_{k=1}^{m} \zeta_k p_k (\mathbfit{x}), \]
%however, in most practical applications, radial kernels are effectively used without polynomial augmentation, typically producing satisfactory results while avoiding matrix singularity issues. For this reason, the additional polynomial terms are commonly omitted.
%
%Following this classification scheme, we define the hybrid radial kernel (HRK) family as:
%\begin{equation}\label{eq12}
%\psi_j(\mathbfit{x})=\alpha\Phi_j^\varepsilon(\mathbfit{x})+\beta\varphi_j(\mathbfit{x}), \quad j=1,2,...,n,
%\end{equation}
%where $\Phi^\varepsilon(\mathbfit{x})=\phi(\varepsilon\Vert \mathbfit{x}\Vert)$ is an infinitely smooth  radial  kernel with shape parameter $\varepsilon$ and $\varphi(\mathbfit{x})=\varpi(\Vert \mathbfit{x} \Vert)$ is a piecewise smooth  radial  kernel free from shape parameter. 
%The coefficients  $\alpha$ and $\beta$ are positive real numbers that control the contribution of each kernel. Since scaling a radial kernel by a constant does not affect the algorithm \cite{43}, the hybrid kernel family (\ref{eq12}) can be normalized using the ratio $\rho=\dfrac{\beta}{\alpha}$. This leads to
%\begin{equation}\label{eq13}
%\psi_j^{\varepsilon,\rho}(\mathbfit{x})=\Phi_j^\varepsilon(\mathbfit{x})+\rho\varphi_j(\mathbfit{x}), \quad j=1,2,...,n.
%\end{equation}
%The HRKs family (\ref{eq13}) incorporates two key parameters the shape parameter $\varepsilon$ and the weight parameter  $\rho$, which govern the trade-off between accuracy and stability in the hybrid kernel method. Next, we approximate the unknown function $u(\mathbfit{x})$  using HRKs by defining:
%
%\begin{equation}\label{eq14}
%u(\mathbfit{x})\approx\mathcal{Q}_nu(\mathbfit{x})=\sum_{j=1}^n c_j\psi_j^{\varepsilon, \rho}(\mathbfit{x}), \quad \mathbfit{x} \in \Omega\subset\mathbb{R}^d,
%\end{equation}
%where
%\begin{equation}\label{eq15}
%\psi_j^{\varepsilon, \rho}(\mathbfit{x})=\Phi_j^\varepsilon(\mathbfit{x})+\rho\varphi_j(\mathbfit{x})=\phi_j(\varepsilon\Vert \mathbfit{x}-\mathbfit{x}_j\Vert)+\rho \varpi_j(\Vert \mathbfit{x}-\mathbfit{x}_j\Vert), \quad j=1,2,...,n.
%\end{equation}
%The coefficients  $\mathbf{c}=[c_1,c_2,...,c_n]^T$ are calculated by solving  $\mathbf{\Gamma c=u}$, where the hybrid  kernel matrix $\mathbf{\Gamma} \in \mathbb{R}^{n \times n}$ is given by 
%\begin{align*}
%\mathbf{\Gamma}_{i,j}=\psi_j^{\varepsilon,\rho}(\mathbfit{x}_i), \quad i,j=1, \cdots, n.
%\end{align*}
%\begin{table}
%\centering
%\caption{Some well-known  radial kernels}\label{tab1}
%\vspace*{0cm}
%\begin{tabular}{lllllll}
%\hline
%\hline
%\textbf{Name of radial kernel} &&  && \textbf{Definition }\\
%\hline
%{\textbf{Infinitely smooth:}} && && &&\\
%\\
%{Multiquadrics (MQ)} &&  && {$\sqrt{\varepsilon^2r^2+1}$}\\
%\\
%{Inverse multiquadrics (IMQ)} &&  && {$\dfrac{1}{\sqrt{\varepsilon^2r^2+1}}$}\\
%\\
%{Gaussian (GA)} &&  && {$e^{-(\varepsilon r)^2}$}\\
%\\
%\hline
%{\textbf{Piecewise smooth:}} && && && \\
%\\
%{Thin plate spline (TPS)} &&  && {$ r^{2} \log(r)$}\\
%\\
%{Cubic (CU)}  &&  && {$r^{3}$}\\
%\hline
%\hline
%\end{tabular}
%\end{table}
%\subsection{Error analysis for HRKs}
%Here, we will delve into the error bound  of HRKs interpolation. In order to do so, it is necessary to introduce specific Hilbert spaces that are related to the radial kernels known as native Hilbert spaces.
%\begin{theorem}\cite{31, 44}\label{theoo1}
%Let $\Phi\in L_1(\mathbb{R}^d) \cap C(\mathbb{R}^d) $ be a real valued strictly positive definite kernel. Then the real native Hilbert space respect to $\Phi$ is 
%\begin{align*}
%\mathcal{N}_\Phi(\mathbb{R}^d)=\bigg\lbrace f\in    L_2(\mathbb{R}^d) \cap C(\mathbb{R}^d):\dfrac{\hat{f}}{\sqrt{\hat{\Phi}}}\in L_2(\mathbb{R}^d)\bigg\rbrace,
%\end{align*}
%with inner product
%\begin{align*}
%\langle f,g \rangle_{\mathcal{N}_{\Phi(\mathbb{R}^d)}}=(2\pi)^{-\frac{d}{2}}\langle\frac{\hat{f}}{\sqrt{\hat{\Phi}}},\frac{\hat{g}}{\sqrt{\hat{\Phi}}}\rangle_{L_{2(\mathbb{R}^d)}}=(2\pi)^{-\frac{d}{2}}\int_{\mathbb{R}^d}\frac{\hat{f}(w)\overline{\hat{g}(w)}}{\sqrt{\hat{\Phi}(w)}}dw,
%\end{align*}
%where $\hat{f}$ is the Fourier transform of $f$.
%\end{theorem}
%We can also extend the notion of native spaces for conditionally positive definite radial kernels, although the specific intricacies will not be discussed  in this study. (for additional information, refer to \cite{31, 44}).
%
%To obtain an error bound, we will now  present two common indicators of data regularity \cite{31, 44}:
%\begin{definition}\label{deff2}
% The   fill distance  of  $\mathit{X}=\{ \mathbfit{x}_i\}_{i=1}^{n} \subset \Omega$ is described by
%\begin{equation}\label{eq16}
%h_{\mathit{X},\Omega}:=\sup_{\mathbf{\mathbfit{x}}\in \Omega}\min_{1\leq j \leq n}\Vert \mathbfit{x}-\mathbfit{x}_j\Vert_2.
%\end{equation}
%\end{definition}
%\begin{definition}\label{deff3}
%The separation distance of $\mathit{X}=\{ \mathbfit{x}_i\}_{i=1}^{n}$ is described by
%\begin{equation}\label{eq17}
%q_\mathit{X}:=\dfrac{1}{2}\min_{i\neq j}\Vert \mathbfit{x}_i-\mathbfit{x}_j \Vert.
%\end{equation}
%\end{definition}
%\begin{remark}
%The values  (\ref{eq16}) and (\ref{eq17}) describe an idea of the data distribution,  indicating the level of uniformity among nodes. In fact, a set $\mathit{X}$ of data sites is said to be quasi-uniform with respect to a constant $c_{qu}>0$ if
%\[q_{\mathit{X}} \leq h_{\mathit{X},\Omega} \leq c_{qu}q_{\mathit{X}}.\]
%\end{remark}
%
%Based on the definitions provided, we can now present the following theorem:
%\begin{theorem}\label{theoo2}\cite{31, 44}
%Let $\Omega \subset \mathbb{R}^d  $ be bounded and satisfy an interior cone condition.
%If   $\Phi$ is conditionally positive deﬁnite  and denote the interpolant to $u \in\mathcal{n}_{\Phi}(\Omega)$ based on
%distant nodal points $\mathit{X}=\{ \mathbfit{x}_i\}_{i=1}^{n}$ by 
%$ \mathcal{P}_{n}u$, and if the derivatives of $\Phi$ of order $2k$ are continuous on
%$\Omega \times \Omega,$   then there are constants $h_0$ and $C$ such that
%\begin{equation}\label{eq18}
%\Vert u- \mathcal{P}_{n}u \Vert_{L^{\infty}(\Omega)} \leqslant C \sqrt{C_\Phi} h_{{}_{\mathit{X} , \Omega}}^{k}\Vert u \Vert _{\mathcal{N}_{\Phi}(\Omega)},
%\end{equation}
%provided  $h_{{}_{\mathit{X} , \Omega}} \leqslant h_0.$ The number $C_\Phi$ is determined by
%\[ C_\Phi= \max_{\vert \gamma \vert=2k} \max_{\varsigma , z \in \Omega \cap B(\mathbfit{x}, c_2h_{{}_{\mathit{X} , \Omega}})} \vert  \mathcal{D}_{2}^{\gamma}\Phi(\varsigma, z)  \vert. \]
%\end{theorem}
%The aforementioned theorem allows us to  describe an error bound for approximation achieved through hybrid kernels.
%\begin{corollary}\label{cor1}
%We know that any  linear combination of conditionally positive definite kernels with nonnegative coefficients gives a conditionally positive definite \cite{31}.
%Therefore, since  $\Phi^{\varepsilon}( \mathbfit{x} )$  and $\varphi( \mathbfit{x})$  are   conditionally  strictly positive definite kernels  then $\psi^{\varepsilon, \rho}(\mathbfit{x})=\Phi^{\varepsilon}(\mathbfit{x})+\rho \varphi(\mathbfit{x})$ is conditionally positive deﬁnite for all $\rho>0.$
%Hence, according to Theorem (\ref{theo2}),  for the  hybrid kernel $\psi^{\varepsilon, \rho}(\mathbfit{x})  \in C^{2k}(\Omega \times \Omega) $, two positive constants $\tilde{h}_0$ and $\tilde{C}$ are exist such that
%\begin{equation}\label{eq19} 
%\Vert u- \mathcal{Q}_{n}u \Vert_{L^{\infty}(\Omega)} \leqslant \tilde{C} \sqrt{\tilde{C}_{\psi^{\varepsilon, \rho}}} h_{{}_{\mathit{X} , \Omega}}^{k}\Vert u \Vert _{\mathcal{N}_{{\psi^{\varepsilon, \rho}}}(\Omega)}
%\end{equation}
%where $u \in\mathcal{n}_{{\psi^{\varepsilon, \rho}}}(\Omega)$, interpolation of function $u$ on the set $\mathit{X}$ denotes with $\mathcal{Q}_{n}u,$ $\mathcal{N}_{{\psi^{\varepsilon, \rho}}}(\Omega)$ is the native space of kernel ${\psi^{\varepsilon, \rho}( \mathbfit{x} )},$ $h_{{}_{\mathit{X} , \Omega}} \leqslant \tilde{h}_0$ and
%\[ \tilde{C}_{\psi^{\varepsilon, \rho}}= \max_{\vert \gamma \vert=2k} \max_{\varsigma , z \in \Omega \cap B(\mathbfit{x}, c_2h_{{}_{\mathit{X} , \Omega}})} \vert  \mathcal{D}_{2}^{\gamma}{\psi^{\varepsilon, \rho}}(\varsigma, z)  \vert.  \]
%\end{corollary}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%To approximate a function $u(x)$ in (\ref{equ1}) using HRKs, we define \cite{a57} $\Psi^{ \varepsilon,\rho}(\textbf{x})=\psi^{ \varepsilon,\rho}(\Vert\textbf{x}\Vert)$
%
%\begin{equation}\label{equ300}
%u(\textbf{x})\approx\mathcal{P}_Nu(\textbf{x})=\sum_{j=1}^n c_j \psi^{ \varepsilon,\rho}(\Vert\textbf{x}-\textbf{x}_j\Vert), \quad \textbf{x}\in\Omega,
%\end{equation}
%where  represents a set of distinct nodes within $\{\textbf{x}_1,\dots,\textbf{x}_n\}\subset\Omega\subset\mathbb{R}^d$, and the coefficients  $\{c_1,\dots, c_n\}$ are determined based on the interpolation conditions:
%
%\begin{equation}\label{equ301}
%\mathcal{P}_Nu(\textbf{x}_i)=u(\textbf{x}_i)=u_i, \quad i=1,\dots, n.
%\end{equation}
%
%Nonlinear Volterra integral equations have significant applications in various fields. For instance:
%\begin{itemize}
%\item
%A nonlinear Volterra integral equation appears in the study of neural networks involving post-inhibitory rebound \cite{a11, a12, a58}
%
%\begin{equation}\label{equ2}
%u(x)=1+\int_{0}^{x}(x-y)^3(4-x+y)e^{(y-x)}\dfrac{u^4(y)}{1+2u^2(y)+2u^4(y)}dy,\quad x\in[0,10],
%\end{equation}
%
%\item
%Another example emerges in the study of sound wave reflection \cite{a11, a12, a58, a59}
%
%\begin{equation}\label{equ3}
%u(x)=f(x)-\int_{0}^x \dfrac{2}{(x-y+2)^2}g(u(y))dy, \quad x\in[0,40]
%\end{equation}
%
%\item
%Additionally, certain types of Volterra integral equations reformulate initial value problems in mathematical physics \cite{a60, a61, a62}. A notable example is the Lane–Emden equation, which describes gravitational potentials in white dwarfs, isothermal gas spheres, and stellar models in Newtonian gravity:
%
%\begin{equation}\label{equ4}
%s^{''}+\dfrac{2}{t}s^{'}+f(t)g(s)=h(t), \quad 0<t<\infty,
%\end{equation}
%
%%subject to
%
%\begin{equation}\label{equ5}
%s(0)=a,\quad s^{'}(0)=0,
%\end{equation}
%
%%where $a$ is a constant and $f$ , $g$ and $h$ are determined functions. The given Lane–Emden problem can be transformed into the following Volterra integral equation [**]
%This problem can be transformed into a corresponding Volterra integral equation \cite{a60, a61}
%
%\begin{equation}\label{equ6}
%s(x)=a+\int_0^{x}(\dfrac{y^2}{x}-y)(f(y)g(s(y))-h(y))dy,\quad x\in(0,\infty).
%\end{equation}
%
%\end{itemize}
%
%For further examples of Volterra integral equations and their applications, refer to \cite{a63}.

\section{Theoretical preliminaries of hybrid kernels}\label{sec2}

This section presents the essential definitions and mathematical foundations of HRKs used in this study.

\subsection{{\color{red} Interpolation} by HRKs}

Hybrid radial kernels  provide a flexible framework for approximating functions in any dimension. We begin by defining radial kernels.

\begin{definition}\cite{44}\label{deff1}
A kernel $\Phi: \mathbb{R}^d \to \mathbb{R}$ is termed radial if there exists a univariate function $\phi: [0, \infty) \to \mathbb{R}$ such that 
\[
\Phi(\mathbf{x}) = \phi(r),
\]
where $r = \|\mathbf{x}\|_2$ and $\|\cdot\|_2$ is the Euclidean norm on $\mathbb{R}^d$.
\end{definition}

Let $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n$ be a set of distinct nodes in a region $\Omega \subset \mathbb{R}^d$. To approximate a function $u(\mathbf{x})$ at a point $\mathbf{x} \in \Omega$ using the radial kernel $\Phi(\mathbf{x})$, we consider the linear combination:
\begin{equation}\label{eq10}
u(\mathbf{x}) \approx \mathcal{P}_n u(\mathbf{x}) = \sum_{j=1}^n \alpha_j \phi(\|\mathbf{x} - \mathbf{x}_j\|_2), \quad \mathbf{x} \in \Omega.
\end{equation}
The coefficients $\boldsymbol{\alpha} = [\alpha_1, \alpha_2, \dots, \alpha_n]^T$ are determined by the interpolation conditions:
\begin{equation}
\mathcal{P}_n u(\mathbf{x}_i) = u(\mathbf{x}_i), \quad i = 1, 2, \dots, n,
\end{equation}
which lead to the linear system:
\begin{equation}\label{eq11}
\mathbf{A} \boldsymbol{\alpha} = \mathbf{u},
\end{equation}
where $\mathbf{A} = [\phi(\|\mathbf{x}_i - \mathbf{x}_j\|_2)]_{i,j=1}^n$ and $\mathbf{u} = [u(\mathbf{x}_1), u(\mathbf{x}_2), \dots, u(\mathbf{x}_n)]^T$.

Radial kernels can be classified into two main categories: infinitely smooth and piecewise smooth kernels, as shown in Table \ref{tab1}. Infinitely smooth kernels, such as Gaussian (GA) and inverse multiquadrics (IMQ), yield positive definite coefficient matrices $\mathbf{A}$ in \eqref{eq11}. Multiquadrics (MQ) produce matrices with one positive eigenvalue and the rest negative, ensuring invertibility \cite{a50, 31, 37, 39, 40, 42}. Piecewise smooth kernels, such as thin plate splines (TPS) and cubic (CU), are symmetric and conditionally strictly positive definite. {\color{red} Therefore, lower degree polynomials  \[     \Lambda(\mathbfit{x})=\sum_{k=1}^{m} \zeta_k p_k (\mathbfit{x}), \] are often required to ensure invertibility. In the above equation, the polynomials $p_1, \cdots, p_m$ form a basis for the $m=\displaystyle \left( \begin{array}{c} d+r-1    \\ r-1 \end{array} \right)$-dimensional linear space  $\displaystyle \Pi_{r-1}^{d}(\Omega)$  of polynomials of total degree less than or equal to $m - 1$ in $d$ variables on the set $\Omega$ \cite{ 44}.  
However, in most cases, radial kernels without augmented polynomials $\Lambda(\mathbfit{x})$ are usually used where satisfactory results have been obtained without encountering singular matrices \cite{a55}. For this reason, the augmented polynomial terms are often removed \cite{a56, a700}. }

Based on the aforementioned categorization, we define the hybrid radial kernel (HRK) family as:
\begin{equation}\label{eq12}
\psi_j(\mathbf{x}) = \alpha \Phi_j^\varepsilon(\mathbf{x}) + \beta \varphi_j(\mathbf{x}), \quad j = 1, 2, \dots, n,
\end{equation}
where $\Phi^\varepsilon(\mathbf{x}) = \phi(\varepsilon \|\mathbf{x}\|_2)$ is an infinitely smooth radial kernel with shape parameter $\varepsilon$, and $\varphi(\mathbf{x}) = \varpi(\|\mathbf{x}\|_2)$ is a piecewise smooth radial kernel without a shape parameter. The coefficients $\alpha$ and $\beta$ are positive real numbers controlling the contribution of each kernel. Since scaling a radial kernel by a constant does not affect the interpolation algorithm \cite{43}, we normalize the HRK family using the ratio $\rho = \dfrac{\beta}{\alpha}$, leading to:
\begin{equation}\label{eq13}
\psi_j^{\varepsilon, \rho}(\mathbf{x}) = \Phi_j^\varepsilon(\mathbf{x}) + \rho \varphi_j(\mathbf{x}), \quad j = 1, 2, \dots, n.
\end{equation}
The HRK family incorporates two parameters: the shape parameter $\varepsilon$ and the weight parameter $\rho$, which balance accuracy and stability in the hybrid kernel method.

To approximate the unknown function $u(\mathbf{x})$ using HRKs, we define:
\begin{equation}\label{eq14}
u(\mathbf{x}) \approx \mathcal{Q}_n u(\mathbf{x}) = \sum_{j=1}^n c_j \psi_j^{\varepsilon, \rho}(\mathbf{x}), \quad \mathbf{x} \in \Omega \subset \mathbb{R}^d,
\end{equation}
where
\begin{equation}\label{eq15}
\psi_j^{\varepsilon, \rho}(\mathbf{x}) = \Phi_j^\varepsilon(\mathbf{x}) + \rho \varphi_j(\mathbf{x}) = \phi(\varepsilon \|\mathbf{x} - \mathbf{x}_j\|_2) + \rho \varpi(\|\mathbf{x} - \mathbf{x}_j\|_2), \quad j = 1, 2, \dots, n.
\end{equation}
The coefficients $\mathbf{c} = [c_1, c_2, \dots, c_n]^T$ are obtained by solving the linear system $\mathbf{\Gamma} \mathbf{c} = \mathbf{u}$, where the hybrid kernel matrix $\mathbf{\Gamma} \in \mathbb{R}^{n \times n}$ is given by:
\[
\mathbf{\Gamma}_{i,j} = \psi_j^{\varepsilon, \rho}(\mathbf{x}_i), \quad i, j = 1, \dots, n.
\]

\begin{table}[ht]
\centering
\caption{Some well-known radial kernels}\label{tab1}
\begin{tabular}{ll}
\hline
\hline
\textbf{Name of radial kernel} & \textbf{Definition} \\
\hline
\textbf{Infinitely smooth:} & \\
Multiquadrics (MQ) & $\sqrt{\varepsilon^2 r^2 + 1}$ \\
Inverse multiquadrics (IMQ) & $\dfrac{1}{\sqrt{\varepsilon^2 r^2 + 1}}$ \\
Gaussian (GA) & $e^{-(\varepsilon r)^2}$ \\
\hline
\textbf{Piecewise smooth:} & \\
Thin plate spline (TPS) & $r^2 \log(r)$ \\
Cubic (CU) & $r^3$ \\
\hline
\hline
\end{tabular}
\end{table}

\subsection{Error Analysis for HRKs}

To establish error bounds for HRK interpolation, we introduce the concept of native Hilbert spaces associated with radial kernels.

\begin{theorem}\cite{44, 31}\label{theoo1}
Let $\Phi \in L_1(\mathbb{R}^d) \cap C(\mathbb{R}^d)$ be a strictly positive integer with a real value. The corresponding real native Hilbert space associated with $\Phi$ can be expressed as:
\[
\mathcal{N}_\Phi(\mathbb{R}^d) = \left\{ f \in L_2(\mathbb{R}^d) \cap C(\mathbb{R}^d) : \dfrac{\hat{f}}{\sqrt{\hat{\Phi}}} \in L_2(\mathbb{R}^d) \right\},
\]
with the inner product:
\[
\langle f, g \rangle_{\mathcal{N}_\Phi(\mathbb{R}^d)} = (2\pi)^{-\frac{d}{2}} \int_{\mathbb{R}^d} \dfrac{\hat{f}(\omega) \overline{\hat{g}(\omega)}}{\hat{\Phi}(\omega)} \, d\omega,
\]
where $\hat{f}$ is the Fourier transform of $f$.
\end{theorem}

The notion of native spaces extends to conditionally positive definite radial kernels; for details, see \cite{44, 31}. To quantify the quality of the approximation, we define two indicators of data regularity:

\begin{definition}\label{deff2}
For a set of nodes $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n$ contained in $\Omega$, the fill distance is defined by: 
\[
h_{\mathcal{X}, \Omega} := \sup_{\mathbf{x} \in \Omega} \min_{1 \leq j \leq n} \|\mathbf{x} - \mathbf{x}_j\|_2.
\]
\end{definition}

\begin{definition}\label{deff3}
The separation distance of $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n$ is:
\[
q_\mathcal{X} := \dfrac{1}{2} \min_{i \neq j} \|\mathbf{x}_i - \mathbf{x}_j\|_2.
\]
\end{definition}

\begin{remark}
A set $\mathcal{X}$ is quasi-uniform with respect to a constant $c_{qu} > 0$ if:
\[
q_{\mathcal{X}} \leq h_{\mathcal{X}, \Omega} \leq c_{qu} q_{\mathcal{X}}.
\]
\end{remark}

The following theorem provides an error bound for interpolation using conditionally positive definite kernels:

\begin{theorem}\label{theoo2}\cite{44, 31}
Suppose $\Omega \subset \mathbb{R}^d$ is a bounded domain satisfying an interior cone condition. Let $\Phi$ be conditionally positive definite, and denote by $\mathcal{P}_n u$ the interpolant of $u \in \mathcal{N}_\Phi(\Omega)$ constructed on {\color{red}the quasi-uniform set } $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n$. If all derivatives of $\Phi$ up to order $2k$ are continuous on $\Omega \times \Omega$, then there exist constants $h_0>0$ and $C>0$ such that: 
\[
\| u - \mathcal{P}_n u \|_{L^\infty(\Omega)} \leq C \sqrt{C_\Phi} h_{\mathcal{X}, \Omega}^k \| u \|_{\mathcal{N}_\Phi(\Omega)},
\]
provided that $h_{\mathcal{X}, \Omega} \leq h_0$, where:
\[
C_\Phi = \max_{|\gamma| = 2k} \max_{\mathbf{s}, \mathbf{z} \in \Omega \cap B(\mathbf{x}, c_2 h_{\mathcal{X}, \Omega})} \left| D_2^\gamma \Phi(\mathbf{s}, \mathbf{z}) \right|.
\]
\end{theorem}

Applying this to hybrid kernels, we have:

\begin{corollary}\label{cor1}
Since $\Phi^\varepsilon(\mathbf{x})$ and $\varphi(\mathbf{x})$ are strictly and conditionally  positive definite kernels, their linear combination $\psi^{\varepsilon, \rho}(\mathbf{x}) = \Phi^\varepsilon(\mathbf{x}) + \rho \varphi(\mathbf{x})$ is conditionally positive definite for all $\rho > 0$ \cite{31}. Thus, by Theorem \ref{theoo2}, if $\psi^{\varepsilon, \rho} \in C^{2k}(\Omega \times \Omega)$, there exist positive constants $\tilde{h}_0$ and $\tilde{C}$ such that:
\[
\| u - \mathcal{Q}_n u \|_{L^\infty(\Omega)} \leq \tilde{C} \sqrt{\tilde{C}_{\psi^{\varepsilon, \rho}}} h_{\mathcal{X}, \Omega}^k \| u \|_{\mathcal{N}_{\psi^{\varepsilon, \rho}}(\Omega)},
\]
where $u \in \mathcal{N}_{\psi^{\varepsilon, \rho}}(\Omega)$, $\mathcal{Q}_n u$ is the HRK interpolation of $u$ on $\mathcal{X}$, $\mathcal{N}_{\psi^{\varepsilon, \rho}}(\Omega)$ is the native space of $\psi^{\varepsilon, \rho}$, $h_{\mathcal{X}, \Omega} \leq \tilde{h}_0$, and:
\[
\tilde{C}_{\psi^{\varepsilon, \rho}} = \max_{|\gamma| = 2k} \max_{\mathbf{s}, \mathbf{z} \in \Omega \cap B(\mathbf{x}, c_2 h_{\mathcal{X}, \Omega})} \left| D_2^\gamma \psi^{\varepsilon, \rho}(\mathbf{s}, \mathbf{z}) \right|.
\]
\end{corollary}

\section{Meshless Implementation for Solving VIEs by HRKs}\label{sec3}

In this part, we develop a meshfree approach based on hybrid radial kernels to tackle nonlinear VIEs of the second kind. Extending prior radial kernel methods, this approach incorporates HRKs to enhance stability and accuracy, addressing limitations of traditional kernels.

Define the operator $\mathcal{F}: C(\Omega) \to C(\Omega)$ as:
\begin{equation}\label{equ7}
\mathcal{F}u = \lambda \int_{a_1}^{x_1} \dots \int_{a_d}^{x_d} K(\mathbf{x}, \mathbf{y}, u(\mathbf{y})) + f(\mathbf{x}) \, d\mathbf{y}, \quad \mathbf{x}, \mathbf{y} \in \Omega,
\end{equation}
where $\Omega = [a_1, b_1] \times \dots \times [a_d, b_d] \subset \mathbb{R}^d$, and {\color{red} $K$ satisfies the following  Lipschitz condition  \cite{a64}:
\[  |K(\mathbf{x}, \mathbf{y}, u_1) - K(\mathbf{x}, \mathbf{y}, u_2)| \leq L_1 |u_1 - u_2|,  \]
  for all $\mathbf{x}, \mathbf{y} \in \Omega$ and $u_1, u_2 \in \mathbb{R}$ with the Lipschitz constant  $L_1$ being independent of $u_1$ and $u_2$.}  Using the operator (\ref{equ7}), we can rewrite (\ref{equ1}) as: 
\begin{equation}\label{equ700}
u=\mathcal{F}u.
\end{equation}
%(\ref{equ8})
Given $n$ collocation points $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n$ in $\Omega$, the solution is approximated as:
\begin{equation}\label{equ9}
u_n(\mathbf{x}) = \sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}), \quad \mathbf{x} \in \Omega,
\end{equation}
where $\psi_j^{ \varepsilon, \rho}(\mathbf{x}) = \psi^{ \varepsilon, \rho}(\|\mathbf{x} - \mathbf{x}_j\|_2)$ are HRKs (see Section \ref{sec2}). The coefficients $\{c_j\}$ satisfy:
\begin{equation}\label{equ10}
\sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}_i) - \lambda \int_{a_1}^{x_{i1}} \dots \int_{a_d}^{x_{id}} K\left(\mathbf{x}_i, \mathbf{y}, \sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\mathbf{y})\right) d\mathbf{y} = f(\mathbf{x}_i), \quad i = 1, \dots, n.
\end{equation}
Integrals are approximated numerically, with techniques varying by dimension.

\subsection{One-Dimensional VIEs}

For $\Omega = [a, b]$, the integral in (\ref{equ10}) is approximated using a  $m$-point composite Gauss-Legendre (CGL) quadrature with $P$ subdivisions, yielding:
\begin{equation}\label{equ16}
\sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(x_i) - \lambda \sum_{q=1}^P \sum_{k=1}^{m} w_k \frac{\Delta y(x_i)}{2} K\left(x_i, \theta_k^q(x_i), \sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(\theta_k^q(x_i))\right) = f(x_i),
\end{equation}
where $\Delta y(x_i) = \frac{x_i - a}{P}$, $\theta_k^q(x_i) = \frac{\Delta y(x_i)}{2} v_k + (q - \frac{1}{2}) \Delta y(x_i)$, and $\{v_k, w_k\}$ are Gauss-Legendre nodes and weights on $[-1, 1]$. The HRKs’ differentiability ensures quadrature accuracy. After solving (\ref{equ16}), the approximate solution is determined to be
\begin{equation}
\hat{u}_{mn}(x)=\sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(x), \quad  x \in \Omega.
\end{equation}

\subsection{Two-Dimensional VIEs}

In two dimensions, e.g., $\Omega = [a, b] \times [c, d]$, VIEs may relate to problems like the Darboux hyperbolic PDE. The double integral in \eqref{equ10} is approximated via tensor-product CGL  quadrature, resulting in:
\begin{align}\label{equ26}
\sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(x_i, t_i) & - \lambda \sum_{q=1}^P \sum_{k=1}^{m} w_k \frac{\Delta y(x_i)}{2} \sum_{b=1}^P \sum_{r=1}^{m} w_r \frac{\Delta s(t_i)}{2} \nonumber \\
& \times K\left(x_i, t_i, \theta_k^q(x_i), \theta_r^b(t_i), \sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(\theta_k^q(x_i), \theta_r^b(t_i))\right) = f(x_i, t_i),
\end{align}
for $i = 1, \dots, n$, where $\Delta s(t_i) = \frac{t_i - c}{P}$, $\theta_r^b(t_i) = \frac{\Delta s(t_i)}{2} v_r + (b - \frac{1}{2}) \Delta s(t_i)$, and $\psi_j^{ \varepsilon, \rho}(x, t) = \psi^{ \varepsilon, \rho}(\sqrt{(x - x_j)^2 + (t - t_j)^2})$. Ultimately, by solving (\ref{equ26}) for the unknowns $\{\hat{c}_j\}_{j=1}^{n}$, the values of $u(x,t)$ can be evaluated by
\[    \hat{u}_{mn}(x,t)= \sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(x, t). \]
\subsection{Higher-Dimensional VIEs}

For $d \geq 3$, the $d$-dimensional integral in (\ref{equ10}) uses $d$-fold CGL quadrature, producing:
\begin{equation}\label{eq240}
\sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}_i) - \lambda \sum_{q=1}^P \sum_{k=1}^{m} \tilde{\mathbf{w}}_k K\left(\mathbf{x}_i, \boldsymbol{\theta}_k^q, \sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(\boldsymbol{\theta}_k^q)\right) = f(\mathbf{x}_i),
\end{equation}
where $\{\tilde{\mathbf{w}}_k\}$ and $\{\boldsymbol{\theta}_k^q\}$ are $d$-dimensional weights and nodes. 
Solving this system,  ultimately  leads to the following numerical solution:
\begin{equation}
\hat{u}_{mn}(\mathbf{x})=\sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}), \quad  \mathbf{x} \in \Omega.
\end{equation}
The HRK-based collocation method consistently balances accuracy and stability across dimensions, as explored further in subsequent sections.
\begin{algorithm}
\caption{Meshless HRK-Based Collocation Method for Solving VIEs}
\begin{algorithmic}[1]
\Require Domain $\Omega = [a_1, b_1] \times \dots \times [a_d, b_d] \subset \mathbb{R}^d$, number of collocation points $n$, HRK shape parameter $ \varepsilon$, weight parameter $\rho$, base kernels $\Phi^ \varepsilon(\mathbf{x})$ (infinitely smooth) and $\varphi(\mathbf{x})$ (piecewise smooth), functions $f(\mathbf{x})$ and $K(\mathbf{x}, \mathbf{y}, u)$, quadrature parameters $m$ (points) and $P$ (subdivisions)
\Ensure Approximate solution $\hat{u}_{mn}(\mathbf{x})$ for any $\mathbf{x} \in \Omega$

\State \textbf{Select Collocation Points:} Choose $n$ distinct points $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n \subset \Omega$ (e.g., uniformly spaced or scattered).
\State \textbf{Define Hybrid Radial Kernels (HRKs):} For each $\mathbf{x}_j$, define
\[
\psi_j^{ \varepsilon, \rho}(\mathbf{x}) = \Phi^ \varepsilon(\mathbf{x} - \mathbf{x}_j) + \rho \varphi(\mathbf{x} - \mathbf{x}_j),
\]
in which $\Phi^ \varepsilon$ is an infinitely smooth kernel (e.g., Gaussian) and $\varphi$ is a piecewise smooth kernel (e.g., cubic spline).
\State \textbf{Set Up the Collocation System:} Approximate the solution as
\[
u_n(\mathbf{x}) = \sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}),
\]
and enforce the VIE at each $\mathbf{x}_i$:
\[
\sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}_i) - \lambda \int_{a_1}^{x_{i1}} \dots \int_{a_d}^{x_{id}} K\left(\mathbf{x}_i, \mathbf{y}, \sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\mathbf{y})\right) d\mathbf{y} = f(\mathbf{x}_i), \quad i = 1, \dots, n.
\]
\State \textbf{Numerically Integrate the Integral Term:} 
\If {$d = 1$}
    \State Use CGL quadrature with $m$ points and $P$ subdivisions over $[a, x_i]$:
    \[
    \int_a^{x_i} K\left(x_i, t, \sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(t)\right) dt \approx \sum_{q=1}^P \sum_{k=1}^{m} w_k \frac{\Delta y(x_i)}{2} K\left(x_i, \theta_k^q(x_i), \sum_{j=1}^n c_j \psi_j^{ \varepsilon, \rho}(\theta_k^q(x_i))\right),
    \]
    in which $\Delta y(x_i) = \frac{x_i - a}{P}$, $\theta_k^q(x_i) = \frac{\Delta y(x_i)}{2} v_k + \left(q - \frac{1}{2}\right) \Delta y(x_i)$, and $\{v_k, w_k\}$ are Gauss-Legendre nodes and weights.
\ElsIf {$d = 2$}
    \State Use tensor-product CGL quadrature over $[a, x_i] \times [c, t_i]$.
\Else \quad ($d \geq 3$)
    \State Extend to $d$-fold CGL quadrature.
\EndIf
\State \textbf{Form the Nonlinear System:} Substitute the quadrature into the collocation equations to obtain
\[
\mathbf{F}(\mathbf{\hat{c}}) = \mathbf{0},
\]
where $\mathbf{\hat{c}} = [\hat{c}_1, \dots, \hat{c}_n]^T$.
\State \textbf{Solve the Nonlinear System:} Use an iterative solver (e.g., Newton’s method) to find $\mathbf{\hat{c}}$, 
%starting from an initial guess (e.g., $\mathbf{c}^{(0)} = \mathbf{0}$).
\State \textbf{Construct the Approximate Solution:} Compute
\[
\hat{u}_{mn}(\mathbf{x}) = \sum_{j=1}^n \hat{c}_j \psi_j^{ \varepsilon, \rho}(\mathbf{x}).
\]
\State \textbf{(Optional) Optimize Parameters:} Adjust $ \varepsilon$ and $\rho$ (e.g., via particle swarm optimization)
%to minimize error.
\end{algorithmic}
\end{algorithm}


\section{Analytical Convergence Results}\label{sec4}

This part examines the convergence and accuracy of the HRK-based collocation approach for nonlinear VIEs. This discussion is primarily derived from \cite{a65}.  \\
 The collocation operator $\mathcal{Q}_n: \mathcal{N}_{\psi^{ \varepsilon, \rho}}(\Omega) \to C_n(\Omega)$ is given by:
\begin{equation}\label{equ30}
\mathcal{Q}_n u(\mathbf{x}) = \sum_{k=1}^n c_k \psi^{ \varepsilon, \rho}(\|\mathbf{x} - \mathbf{x}_k\|_2), \quad \mathbf{x} \in \Omega,
\end{equation}
in which $C_n(\Omega) = \operatorname{span}\{\psi^{ \varepsilon, \rho}(\|\mathbf{x} - \mathbf{x}_1\|_2), \dots, \psi^{ \varepsilon, \rho}(\|\mathbf{x} - \mathbf{x}_n\|_2)\}$.   \\
The numerical scheme approximates the VIE via:
\begin{equation}\label{equ34}
\mathcal{Q}_n \mathcal{F}_m \hat{u}_{mn} = \hat{u}_{mn},
\end{equation}
where $\mathcal{F}_m$ is a numerical integral operator. Also, for $u \in C^{2m}(\Omega)$ and $K \in C^{2m}(\Omega \times \Omega \times \mathbb{R})$, the error satisfies:
\begin{equation}\label{equ33}
\| \mathcal{F} u - \mathcal{F}_m u \|_{\infty} \leq \frac{C_m}{P^{2m}} \sup_{\mathbf{x} \in \Omega} |u^{(2m)}(\mathbf{x})|,
\end{equation}
with $C_m$ tied to regularity parameters \cite{a67}.

Using the Mean-Value Theorem, for some $\xi(\mathbf{y})$ between $u(\mathbf{y})$ and $\hat{u}_n(\mathbf{y})$:
\begin{equation}\label{equ35}
K(\mathbf{x}, \mathbf{y}, u(\mathbf{y})) - K(\mathbf{x}, \mathbf{y}, \hat{u}_n(\mathbf{y})) = K_u(\mathbf{x}, \mathbf{y}, \xi(\mathbf{y})) (u - \hat{u}_n)(\mathbf{y}).
\end{equation}

Define operators $\mathcal{W}_1, \mathcal{W}_2: C(\Omega) \to C(\Omega)$ via quadrature:
\begin{align}
\mathcal{W}_1 \psi^{ \varepsilon, \rho}(\mathbf{x}) &= \int_{\Omega} K_u(\mathbf{x}, \mathbf{y}, u(\mathbf{y})) \psi^{ \varepsilon, \rho}(\mathbf{y}) \, d\mathbf{y}, \label{equ36} \\
\mathcal{W}_2 \psi^{ \varepsilon, \rho}(\mathbf{x}) &= \int_{\Omega} K_u(\mathbf{x}, \mathbf{y}, \xi(\mathbf{y})) \psi^{ \varepsilon, \rho}(\mathbf{y}) \, d\mathbf{y}, \label{equ37}
\end{align}
approximated numerically as needed.\\
{\color{red} Throughout this paper, we resort to the standard hypothesis, such that $(\mathcal{I} - \mathcal{W}_1)^{-1}$  always exists
and is bounded \cite{a64}.} Based on the above assumptions, as $n \longrightarrow  \infty$  we have  \cite{a64}
\begin{equation}\label{equ38}
\| \mathcal{W}_1 - \mathcal{W}_2 \| \to 0, \quad \| (\mathcal{I} - \mathcal{Q}_n) \mathcal{W}_1 \| \to 0.
\end{equation}

The main convergence result is:

\begin{theorem}\label{theo2}  {\color{red} Assume the new scheme has been constricted on the quasi-uniform set $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^n$ on the domain $\Omega$. }
Under the conditions of Corollary \ref{cor1}, for $u \in \mathcal{N}_{\Psi^{ \varepsilon, \rho}}(\Omega) \cap C^{2m}(\Omega)$ solving the VIE with $K \in C^{2m}(\Omega \times \Omega \times \mathbb{R})$, the operator  $(\mathcal{I} - \mathcal{Q}_n \mathcal{W}_2)^{-1}$ exists and is uniformly bounded for large $n$. The error satisfies:
\begin{align*} 
\Vert \hat{e}_{mn} \Vert 
  \leq   \alpha (  \sigma  \frac{C_m}{P^{2m}} \sup_{\mathbf{x} \in \Omega} |u^{(2m)}(\mathbf{x})|+ \tilde{C} \sqrt{\tilde{C}_{\psi^{\varepsilon, \rho}}} h_{\mathcal{X}, \Omega}^k \| u \|_{\mathcal{N}_{\psi^{\varepsilon, \rho}}(\Omega)}),
\end{align*}
where $\alpha, \sigma, m,\tilde{C}, C_m$ are positive constants.
\end{theorem}
  
\begin{proof}
From (\ref{equ700}), we have
\begin{equation}
 \mathcal{Q}_nu= \mathcal{Q}_n \mathcal{F}u.
\end{equation}
Now utilizing  (\ref{equ34}) leads to 
\begin{equation}
 \mathcal{Q}_nu -  \hat{u}_{mn}= \mathcal{Q}_n (\mathcal{F}u- \mathcal{F}_m \hat{u}_{mn})= \mathcal{Q}_n ( \{ \mathcal{F}u- \mathcal{F}_m u  \} +\{  \mathcal{F}_m u  - \mathcal{F}_m \hat{u}_{mn} \}      ).
\end{equation}
From (\ref{equ35}), we have
\begin{equation}
\mathcal{F}_m u  - \mathcal{F}_m \hat{u}_{mn} = \mathcal{W}_2 \hat{e}_{mn}= \mathcal{W}_2  (u- \hat{u}_{mn}).
\end{equation}
Thus, we can write 
\begin{equation}
(\mathcal{I} -  \mathcal{Q}_n \mathcal{W}_2) \hat{u}_{mn}=  \mathcal{Q}_n (u - \{ \mathcal{F} u -\mathcal{F}_mu \} - \mathcal{W}_2 u),
\end{equation}
or equivalently 
\begin{equation} \label{TT7}
(\mathcal{I} -  \mathcal{Q}_n \mathcal{W}_2) \hat{e}_{mn}= \mathcal{Q}_n ( \mathcal{F} u -\mathcal{F}_mu ) + (\mathcal{Q}_n u- u).
\end{equation}
To demonstrate the existence of $(\mathcal{I} - \mathcal{Q}_n \mathcal{W}_2)^{-1}$, we consider
\begin{align*} 
\mathcal{I} - \mathcal{Q}_n \mathcal{W}_2
& = ( \mathcal{I} - \mathcal{Q}_n \mathcal{W}_1 )+ \mathcal{Q}_n (\mathcal{W}_1 - \mathcal{W}_2) \notag \\
& = (\mathcal{I} -  \mathcal{W}_1)+ (\mathcal{I} - \mathcal{Q}_n) \mathcal{W}_1  + \mathcal{Q}_n (\mathcal{W}_1 - \mathcal{W}_2) \\
& = (\mathcal{I} -  \mathcal{W}_1) \{  \mathcal{I} + (\mathcal{I} -  \mathcal{W}_1)^{-1}  [  (\mathcal{I} - \mathcal{Q}_n) \mathcal{W}_1  + \mathcal{Q}_n (\mathcal{W}_1 - \mathcal{W}_2)  ]         \}.
\end{align*}
From the existence and uniform boundedness of $(\mathcal{I} -  \mathcal{W}_1)^{-1} $ , and by applying (\ref{equ38}), it follows that the operator $(\mathcal{I} - \mathcal{Q}_n \mathcal{W}_2 )^{-1} $ also exists and remains uniformly bounded for sufficiently large $n$, i.e., 
\begin{align}
\Vert  (\mathcal{I} - \mathcal{Q}_n \mathcal{W}_2 )^{-1}  \Vert \leq \alpha,
\end{align}
in which $\alpha$ is a positive constant. Also, from (\ref{TT7}) and the uniform boundedness  of the family  $\{ \mathcal{Q}_n \}$ \cite{assari1}, namely
\[ \sup_{n \geq 1} \Vert \mathcal{Q}_n \Vert_{\infty} \leq \sigma, 
\]
we have
\begin{align*} 
\Vert \hat{e}_{mn} \Vert 
& \leq   \Vert  (\mathcal{I} - \mathcal{Q}_n \mathcal{W}_2 )^{-1}  \Vert  \Vert \mathcal{Q}_n ( \mathcal{F} u -\mathcal{F}_mu ) + (\mathcal{Q}_n u- u) \Vert   \notag \\
&   \leq   \alpha (  \sigma  \Vert \mathcal{F} u -\mathcal{F}_mu  \Vert + \Vert \mathcal{Q}_n u- u \Vert ).
\end{align*}
Since  $u \in \mathcal{N}_{\Psi^{ \varepsilon, \rho}}(\Omega) \cap C^{2m}(\Omega)$ 
from (\ref{equ33}) and Corollary \ref{cor1}, we have 
\begin{align*} 
\Vert \hat{e}_{mn} \Vert 
  \leq   \alpha (  \sigma  \frac{C_m}{P^{2m}} \sup_{\mathbf{x} \in \Omega} |u^{(2m)}(\mathbf{x})|+ \tilde{C} \sqrt{\tilde{C}_{\psi^{\varepsilon, \rho}}} h_{\mathcal{X}, \Omega}^k \| u \|_{\mathcal{N}_{\psi^{\varepsilon, \rho}}(\Omega)}).
\end{align*}   \qed
\end{proof}  

\section{Novel Parameter Selection Methodology}\label{sec5}

This section presents an optimized approach for selecting the shape parameter $ \varepsilon$ and weight parameter $\rho$ in  HRKs, critical for balancing accuracy and stability in the meshless solution of VIEs.

\subsection{Objective Function}

The objective function for parameter optimization is the root-mean-square (RMS) error of the HRK {\color{red} interpolation}:
\[
O_f( \varepsilon, \rho) = \sqrt{\frac{1}{M} \sum_{j=1}^{M} \left[ u(\zeta_j) - \tilde{u}_{n}(\zeta_j;  \varepsilon, \rho) \right]^2},
\]
where $\{\zeta_j\}_{j=1}^M$ are evaluation points in $\Omega$, $u$ is the exact solution, and $\tilde{u}_{n}(\cdot;  \varepsilon, \rho)$ is the HRK-based {\color{red} interpolation} from Section \ref{sec3}. The optimization problem is:
\begin{center}
Minimize  $O_f(\varepsilon, \rho)$ \\
subject to $0 \leq \varepsilon \leq 10,$\\
 \quad \quad \quad \quad  $0 \leq \rho \leq 1,$
\end{center}
where  $\varepsilon$ and $\rho$ are the kernel parameters. {\color{red} The bounds for $\varepsilon$ and $\rho$ were chosen based on theoretical and empirical considerations. Theoretically, very small $\varepsilon$ causes ill-conditioning, while very large $\varepsilon$ reduces accuracy. 
On the other hand, the parameter $\rho$ should be chosen sufficiently large to enhance numerical stability, while remaining small enough to ensure rapid convergence. Empirically, as shown in Section 6, the optimal solutions consistently lie within the chosen ranges $\varepsilon \in  [0,10]$ and $\rho \in  [0,1]$ and enlarging the bounds did not improve accuracy. }



{\color{red}\begin{remark} 
Since the aim of the present paper is the comparison of the results of HRKs method for different VIEs, problems with existing exact solutions are considered. It must be emphasized that for practical problems where the exact solution is not known, it is not possible to calculate the exact RMS norm. In such situations, leave-one-out cross-validation (LOOCV) \cite{Rippa} is a prominent technique that can be used in conjunction with the PSO algorithm to find the optimal values of parameters.
\end{remark} }

\subsection{Parameter Optimization via PSO}

Given the multimodal nature of $O_f$ due to the interplay between $ \varepsilon$ and $\rho$ in HRKs, we employ Particle Swarm Optimization (PSO) \cite{a77}, a metaheuristic algorithm well-suited for non-convex problems. PSO iteratively refines a swarm of candidate solutions based on individual and collective best-known positions.

For HRK parameter selection, we adapt PSO with three modifications to enhance exploration and convergence:

\begin{itemize}
    \item \textbf{Chaotic Initialization:} Particles are initialized using a logistic map \cite{a77} to ensure diversity across the parameter space:
    \[
    z_{j+1} = \mu z_j (1 - z_j), \quad j = 1, 2, \dots,
    \]
    with $\mu = 4$ for full chaos, mapped to $[ \varepsilon_{\text{min}},  \varepsilon_{\text{max}}]$ and $[\rho_{\text{min}}, \rho_{\text{max}}]$.
    
    \item \textbf{Sine Map for Inertia Weight:} The inertia weight $w$ is adjusted dynamically using a sine map \cite{a77} to balance exploration and exploitation:
    \[
    w_t = \frac{\nu}{4} \sin(\pi x_{t-1}), \quad x_t \in (0,1), \quad  0<\nu \leq 4,
    \]
    where $t$ is the iteration number.
    
    \item \textbf{Sine-Cosine Acceleration Coefficients:} Learning coefficients $c_1$ and $c_2$ are varied using sine and cosine functions %\cite{a76]
     to shift focus adaptively from cognitive to social learning:
    \[
    c_1 = 2 + 0.5 \sin\left( \left(1 - \frac{t}{T}\right) \frac{\pi}{2} \right), \quad c_2 = 2 + 0.5 \cos\left( \left(1 - \frac{t}{T}\right) \frac{\pi}{2} \right),
    \]
    where $T$ is the maximum number of iterations~\cite{a77}.
\end{itemize}

These modifications enable PSO to navigate the complex parameter landscape of HRKs effectively, avoiding local minima and accelerating convergence to optimal $( \varepsilon, \rho)$. The procedure is outlined in Figure \ref{a2000}, tailored to HRK parameter optimization.  {\color{red}  Also, the full implementation of the proposed PSO algorithm is publicly available at:
[insert link here].}

\begin{figure}[htp]
\begin{center}
\begin{small}



\tikzstyle{startstop} = [circle,draw, draw=black, fill=red!50]

\tikzstyle{io} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=5cm, 
draw=black, 
fill=violet!50]





\tikzstyle{process} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=6cm, 
draw=black, 
fill=violet!50]



\tikzstyle{process23} = [rectangle, 
minimum width=1cm, 
minimum height=1cm, 
text centered, 
text width=1cm, 
draw=black, 
fill=violet!50]



\tikzstyle{process2} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=7cm, 
draw=black, 
fill=violet!50]


\tikzstyle{process3} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=8cm, 
draw=black, 
fill=violet!50]



\tikzstyle{process4} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=5cm, 
draw=black, 
fill=violet!50]




\tikzstyle{decision} = [diamond, 
minimum width=2cm, 
minimum height=2cm, 
text centered, 
draw=black, 
fill=green!50]

\tikzstyle{process6} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=4cm, 
draw=black, 
fill=violet!50]

\tikzstyle{arrow} = [thick,->,>=stealth]



\begin{tikzpicture}[node distance=1.7cm]

\node (start) [startstop] {Start};
\node (in1) [io, below of=start] {Initialize the particles $\{ \varepsilon_j, \rho_j\}_{j=1}^{n_P} $ based on  chaotic  algorithm};


\node (pro1) [process, below of=in1] {Define parameters of optimization   \\
  $(c_1, c_2, w)$    };
  
  
    \node (pro15) [process23, below of=pro1] {t=0  };
  
  \node (pro11) [process2, below of=pro15] {Evaluate the fitness value of every particle   };
  
  
    \node (pro21) [process3, below of=pro11] {Update velocity  $v_j^{t}$, position $\{ \varepsilon_{j}^{t}, \rho_{j}^{t}\}$ and inertia weight };
  
 
  
    \node (pro31) [process4, below of=pro21] {Update Pbest and Gbest};
  
\node (dec1) [decision, below of=pro31, yshift=-2cm] {Stopping  criterion  met?};




\node (pro2a) [process6, below of=dec1, yshift=-2cm] {Output the final Gbest };


\node (stop) [startstop, below of=pro2a] {End};






\draw [arrow] (start) -- (in1);


\draw [arrow] (in1) -- (pro1);
\draw [arrow] (pro1) -- (pro15);
\draw [arrow] (pro15) -- (pro11);
\draw [arrow] (pro11) -- (pro21);
\draw [arrow] (pro21) -- (pro31);
\draw [arrow] (pro31) -- (dec1);
\draw [arrow] (dec1) -- (pro2a);
\draw [arrow] (pro2a) -- (stop);
\draw [arrow] (dec1) -- node[anchor=east] {Yes} (pro2a);
\draw [arrow] (dec1) -- node[anchor=north] {No, t=t+1} ([xshift=3.6cm] dec1.east) -- ([xshift=2cm] pro11.east) -- (pro11);
\end{tikzpicture}
\end{small}
\end{center}
\vspace*{-0.3cm} \caption{ \small Flowchart illustrating the PSO algorithm implemented in this article.}
 \label{a2000}
\end{figure}



\section{ Numerical verification across dimensional cases}\label{sec6}
This section is devoted to testing the effectiveness and reliability of the hybrid method using numerical examples.
For this study, the following combinations of radial kernels have been employed:
\begin{align*}
\psi_j^{\varepsilon,\rho}(r) &= e^{-(\varepsilon r)^2}+\rho r^3 
\quad (GA+CU), \\
\psi_j^{\varepsilon,\rho}(r) &= \sqrt{\varepsilon^2 r^2+1}+\rho r^3 \quad (MQ+CU), \\
\psi_j^{\varepsilon,\rho}(r)&= e^{-(\varepsilon r)^2}+\rho r^2 \log(r) \quad (GA+TPS), \\ 
 \psi_j^{\varepsilon,\rho}(r)&= \sqrt{\varepsilon^2 r^2+1}+\rho r^2 \log(r) \quad (MQ+TPS).
\end{align*}
In each experiment, the population size was chosen as $n_P=25$ and the iteration limit was fixed at $T=60$.  {\color{red} All calculations and plots have been done by “Maple 18” software and run on a Laptop with 1.70GHz of Core i5-4210U CPU and 6 GB of RAM.}




\begin{example} \label{R12} 
Consider the nonlinear Lane–Emden equation that appears in the study of static stellar models within Newtonian gravity:
\[
y'' + \frac{2}{t} y' + y^5 = 0, \quad 0 < t \leq 1,
\]
subject to 
\[
y(0) = 1, \quad y'(0) = 0,
\]
in which the exact solution is \( y(x) = \left(1 + \frac{x^2}{3}\right)^{-1/2} \). This problem can be expressed as the following VIE \cite{a78}:
\begin{align} \label{2021}
y(x) = 1 + \int_0^x \left( \frac{t^2}{x} - t \right) y^5(t) \, dt, \quad x \in (0, 1).
\end{align}
In this example, to approximate integrals, 5-point CGL integration scheme with $P=5$ has been utilized.
The performance results obtained through the PSO algorithm for various values of 
n are summarized in Table \ref{a55}. {\color{red} Also, Figure \ref{F320} depicts the convergence behavior of the Gbest values associated with $\rho$ and $\varepsilon$  during the optimization process for $n=4$.}
Furthermore, the convergence of the corresponding RMS errors is illustrated in Figure \ref{a55}.
For this figure, the parameters were set to $\varepsilon=0.2$  and  $\rho=10^{-8}$. 
As shown in Figure \ref{F140}, the GA+CU hybrid basis delivers the highest accuracy, outperforming MQ+CU, GA+TPS, and MQ+TPS in sequence.
These findings confirm that hybrid kernels achieve greater accuracy than pure kernels.
Also, Figure \ref{F1400} explores the RMS error behavior as a function of $\varepsilon \in [0.02, 10]$ for varying $\rho$ values, using both the pure GA kernel and GA+CU hybrid kernel with $n=10$ on a log-log scale.
The results indicate that at larger $\rho$ values, the CU term dominates the GA term, which slows convergence. Conversely, smaller $\rho$ values allow the GA term to prevail, resulting in improved accuracy. Notably, the lowest errors generally occur when the weight parameter $\rho$ is small.
In addition, Figure \ref{F1400} highlights that the pure GA kernel ($\rho=0$) loses accuracy when the shape parameter $\varepsilon \lesssim 1.6$. The GA+CU hybrid kernel, however, provides stable approximations across all shape parameter values.  {\color{red} In this algorithm, $\rho$ must be sufficiently large to enhance numerical stability while remaining small enough to allow rapid convergence. Overall, numerical results show that the hybridization of piecewise smooth radial kernels with infinitely smooth radial kernels remarkably reduced ill-conditioning and increased accuracy.}
For comparison purposes, we additionally solved integral equation (\ref{2021}) by applying the HS-SVD method \cite{a50} with kernels of Legendre, Brownian bridge, and Brownian motion type. The results, provided in Table \ref{3a1}, reveal that the proposed method achieves higher convergence rates and requires less CPU time compared to the HS-SVD method, further underscoring its computational efficiency and accuracy.


\begin{table}[!htb] 
\caption{\small Findings from the parameter optimization test for example \ref{R12} employing hybrid kernels.} \vspace*{0.1cm} \scalebox{1}{
\begin{tabular}{lcccccccccccc}\cmidrule(lr){1-11} 
n & & Hybrid kernel & & $\varepsilon_{opt}$ & & $\rho_{opt}$ & & RMS error & & & & \\ \cmidrule(lr){1-11} 
4 & & {GA+CU} & & {$0.3209 $} & & {$5.8012\times 10^{-8}$} & &{$4.4339 \times 10^{-5}$}& &  
\\
& & {MQ+CU} & & {$0.2957$} & & {$1.8755\times 10^{-7} $} & &{$1.7594 \times 10^{-5}$}& & 
\\
& & {GA+TPS} & & {$0.3921 $} & & {$2.0367 \times 10^{-6}$} & &{$4.9284 \times 10^{-5}$}& & 
\\
& & {MQ+TPS} & & {$0.2061$} & & {$7.3254\times 10^{-8} $} & &{$5.8312 \times 10^{-5}$}& &
\\ 
\cmidrule(lr){1-11} 
8 & & {GA+CU} & & {$0.7027 $} & & {$4.2302 \times 10^{-7} $} & &{$2.5242 \times 10^{-8}$}& &  
\\
& & {MQ+CU} & & {$0.3924$} & & {$2.7581 \times 10^{-9} $} & &{$9.8593 \times 10^{-8}$}& &  
\\
& & {GA+TPS} & & {$0.7186$} & & {$3.2482 \times 10^{-9}$} & &{$2.7448 \times 10^{-8}$}& &  
\\
& & {MQ+TPS} & & {$0.5594$} & & {$2.4736 \times 10^{-9} $} & &{$1.7975 \times 10^{-7}$}& & 
\\
\cmidrule(lr){1-11} 
12 & & {GA+CU} & & {$0.9021 $} & & {$7.0068 \times 10^{-10} $} & &{$8.9070 \times 10^{-10}$}& & 
\\
& & {MQ+CU} & & {$0.4275$} & & {$1.1728 \times 10^{-9} $} & &{$1.3281 \times 10^{-9}$}& &
\\
& & {GA+TPS} & & {$0.6954$} & & {$2.5921 \times 10^{-10}$} & &{$4.0667 \times 10^{-9}$}& & 
\\
& & {MQ+TPS} & & {$0.3451$} & & {$3.2103 \times 10^{-10} $} & &{$7.3687 \times 10^{-9}$}& &  
\\
\cmidrule(lr){1-11} 
16 & & {GA+CU} & & {$0.8011 $} & & {$3.0753 \times 10^{-10} $} & &{$2.4463 \times 10^{-11}$}& &  
\\
& & {MQ+CU} & & {$0.4512$} & & {$1.1729 \times 10^{-10} $} & &{$9.8760 \times 10^{-11}$}& & 
\\
& & {GA+TPS} & & {$0.6541$} & & {$4.0025 \times 10^{-10}$} & &{$4.1848 \times 10^{-10}$}& & 
\\
& & {MQ+TPS} & & {$0.4102$} & & {$2.7895 \times 10^{-10} $} & &{$8.9178 \times 10^{-10}$}& & 
\\
\cmidrule(lr){1-11} 
\end{tabular}}
\label{a55}
\end{table}




\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.8\textwidth]{exam1.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{ {\color{red}Convergence of Gbest  values for optimization of $\rho$ and $\varepsilon$ for $n=4$  in Example \ref{R12}.} }
\label{F320} % Give a unique label
\end{figure}






\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.9\textwidth]{2CC.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{RMS error convergence for different kernels.}
\label{F140} % Give a unique label
\end{figure}






\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.9\textwidth]{2DD.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{RMS error convergence for various values of $\rho$.}
\label{F1400} % Give a unique label
\end{figure}


\begin{table}[!htb] 
\caption{\small Errors produced by the HRK  and HS-SVD  approaches in example  \ref{R12} when $n=10$.} \vspace*{0.1cm} \scalebox{0.9}{
\begin{tabular}{lcccccccccccc} \toprule 
Methods & & {} & & {} & & {} & & RMS-error & & CPU Time & & \\ \cmidrule(lr){1-13} 
HS-SVD scheme based on the Legendre kernel & & {} & & {} & & {} & &{$5.452 \times 10^{-3}$}& & {$25.71$} & &
\\
HS-SVD scheme based on the Brownian bridge kernel & & {} & & {} & & {} & &{$6.387 \times 10^{-2}$}& & {$28.41$} & &
\\
HS-SVD scheme based on the Brownian motion kernel & & {} & & {} & & {} & &{$3.712 \times 10^{-3}$}& & {$30.45$} & & 
 \\
GA+CU - enhanced HRKs scheme ($\rho=10^{-8}$, $\varepsilon=1$) & & {} & & {} & & {} & &{$6.656 \times 10^{-7}$}& & {$7.32$} & &
\\
 MQ+CU - enhanced HRKs scheme ($\rho=10^{-8}$, $\varepsilon=1$) & & {} & & {} & & {} & &{$8.321 \times 10^{-7}$}& & {$7.14$} & &
\\
 GA+TPS - enhanced HRKs scheme ($\rho=10^{-8}$, $\varepsilon=1$) & & {} & & {} & & {} & &{$2.078 \times 10^{-6}$}& & {$12.26$} & &
\\
 MQ+TPS - enhanced HRKs scheme ($\rho=10^{-8}$, $\varepsilon=1$) & & {} & & {} & & {} & &{$3.314 \times 10^{-6}$}& & {$11.14$} & &
\\
\bottomrule
\end{tabular}} 
\label{3a1}
\end{table}






\end{example}


\newpage

\newpage




\newpage
\vspace*{5cm}
\newpage
\newpage
\vspace*{5cm}
\newpage

\begin{example}\label{exa1}
Consider the nonlinear VIE \cite{a11}
\begin{align} \label{12a}
u(x)=e^{-x}+ \int_{0}^{x} e^{t-x} (u(t)+ e^{-u(t)}) dt,
\end{align}
with the exact solution $u(x)=\ln(x+e).$ Here, to approximate integrals, 10-point CGL integration scheme with $P=5$ has been utilized.
The performance results obtained through the PSO algorithm for various values of 
n are summarized in Table \ref{a40}. Our observations indicate that the combination of GA and CU kernels demonstrates greater accuracy than other hybrid kernels. {\color{red} Furthermore, Figure \ref{F322} illustrates the convergence behavior  of the Gbest values corresponding to $\rho$ and $\varepsilon$ throughout the optimization process for  $n=4$.}
Also, the absolute errors for $n = 10$ utilizing pure and hybrid kernels are displayed in Figure \ref{DC1}. In this figure, we employed the values $\rho=10^{-10}$ and $\varepsilon=0.2$. 
Furthermore,  Figure \ref{DR1} presents the RMS error values for values $\varepsilon$ between 0.1 and 10, plotted on a log-log scale, for $n=5, 10, 15$ using both GA and GA+TPS kernels.
 In this Figure, $\rho$ is set to $10^{-9}$. 
One can see that the results obtained with GA and GA+TPS kernels overlap when large enough values of $\varepsilon$ are taken into account.
Additionally, as the number of nodes $n$ goes down, the overlapping region expands. Conversely, when $n$ goes up, the overlapping region shrinks.
Moreover, as $\varepsilon$ tends to zero, the numerical results become more unstable when using a pure GA kernel.
However, as compared to the pure GA kernel, the use of the GA and TPS kernel combination shows enhanced stability.
For example, when $n=15,$ the results obtained using the pure GA kernel become unstable once $\varepsilon$ drops below roughly $2.41$. {\color{red}We can
generally conclude that the error is significantly reduced when a small portion of a piecewise smooth radial
kernel is combined with an infinitely smooth radial kernel.}
To evaluate the proposed method, integral equation (\ref{12a}) was also solved utilizing the HS-SVD approach with exponential kernel, and the numerical outcomes are summarized in Table \ref{ab1}. The exponential kernel in a closed form is representation by $\phi(x,t)=e^{(-\varepsilon \vert x-t \vert)}$.
Our findings indicate that the hybrid kernel method converges more quickly than the HS-SVD strategy. In addition, the new approach attains greater accuracy than the HS-SVD mechanism while requiring fewer collocation nodes.







\begin{table}[!htb] 
\caption{\small Findings from the parameter optimization test for example \ref{exa1} employing hybrid kernels.} \vspace*{0.1cm} \scalebox{1}{
\begin{tabular}{lcccccccccccc}\cmidrule(lr){1-11} 
n & & Hybrid kernel & & $\varepsilon_{opt}$ & & $\rho_{opt}$ & & RMS error & & & & \\ \cmidrule(lr){1-11} 
4 & & {GA+CU} & & {$0.3472 $} & & {$3.2418\times 10^{-6}$} & &{$7.3418 \times 10^{-6}$}& &  
\\
& & {MQ+CU} & & {$0.2390 $} & & {$5.8012\times 10^{-6}$} & &{$4.4339 \times 10^{-5}$}& & 
\\
& & {GA+TPS} & & {$0.3476 $} & & {$3.5271 \times 10^{-5}$} & &{$5.9441 \times 10^{-5}$}& & 
\\
& & {MQ+TPS} & & {$0.1491$} & & {$7.3254\times 10^{-6} $} & &{$7.8312 \times 10^{-5}$}& &
\\ 
\cmidrule(lr){1-11} 
8 & & {GA+CU} & & {$0.4041$} & & {$2.2145 \times 10^{-10} $} & &{$2.5411 \times 10^{-8}$}& &  
\\
& & {MQ+CU} & & {$0.2195$} & & {$4.2302 \times 10^{-10} $} & &{$2.9852 \times 10^{-8}$}& & 
\\
& & {GA+TPS} & & {$0.4435$} & & {$8.4123 \times 10^{-9}$} & &{$8.6564 \times 10^{-8}$}& &  
\\
& & {MQ+TPS} & & {$0.2649$} & & {$2.4736 \times 10^{-8} $} & &{$1.9878 \times 10^{-7}$}& & 
\\
\cmidrule(lr){1-11} 
12 & & {GA+CU} & & {$0.5886 $} & & {$1.2746 \times 10^{-10} $} & &{$5.1242 \times 10^{-10}$}& & 
\\
& & {MQ+CU} & & {$0.3460 $} & & {$3.5491 \times 10^{-10} $} & &{$2.8314 \times 10^{-9}$}& & 
\\
& & {GA+TPS} & & {$0.3641$} & & {$9.2158 \times 10^{-10}$} & &{$6.5381 \times 10^{-9}$}& & 
\\
& & {MQ+TPS} & & {$0.3190$} & & {$2.8741 \times 10^{-9} $} & &{$1.6899 \times 10^{-8}$}& &  
\\
\cmidrule(lr){1-11} 
16 & & {GA+CU} & & {$0.5788 $} & & {$1.5231 \times 10^{-10} $} & &{$9.1284 \times 10^{-11}$}& &  
\\
& & {MQ+CU} & & {$0.3476 $} & & {$2.3749 \times 10^{-10} $} & &{$5.8321 \times 10^{-10}$}& & 
\\
& & {GA+TPS} & & {$0.6047$} & & {$7.2475 \times 10^{-10} $} & &{$8.7135 \times 10^{-10}$}& & 
\\
& & {MQ+TPS} & & {$0.5265$} & & {$7.2475 \times 10^{-9} $} & &{$2.5901 \times 10^{-9}$}& & 
\\
\cmidrule(lr){1-11} 
\end{tabular}}
\label{a40}
\end{table}






\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.8\textwidth]{exam2.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{ {\color{red}Convergence of Gbest  values for optimization of $\rho$ and $\varepsilon$ for $n=4$ in Example \ref{exa1}.}}
\label{F322} % Give a unique label
\end{figure}



\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.7 \textwidth]{sin8.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{Absolute error of example \ref{exa1} with $n=10$.}
\label{DC1} % Give a unique label
\end{figure}




\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.9\textwidth]{3FF.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{The RMS error, as a function of $\varepsilon$.}
\label{DR1} % Give a unique label
\end{figure}




\begin{table}[!htb] 
\caption{\small Errors produced by the HRK  and HS-SVD approaches in example \ref{exa1} when $\varepsilon=0.5$.} \vspace*{0.1cm} \scalebox{0.9}{
\begin{tabular}{lcccccccccccc} \toprule 
Methods & & {} & & {} & & {} & & RMS-error & & CPU Time & & \\ \cmidrule(lr){1-13} 
HS-SVD scheme based on the exponential kernel (n=10) & & {} & & {} & & {} & &{$1.366 \times 10^{-3}$}& & {$24.68$} & &
\\
HS-SVD scheme based on the exponential kernel (n=20) & & {} & & {} & & {} & &{$7.411 \times 10^{-4}$}& & {$68.82$} & &
\\
 GA+CU - enhanced HRKs scheme ($\rho=10^{-8}$, n=8) & & {} & & {} & & {} & &{$6.0321 \times 10^{-8}$}& & {$5.76$} & &
\\
 MQ+CU - enhanced HRKs scheme ($\rho=10^{-8}$, n=8) & & {} & & {} & & {} & &{$1.135 \times 10^{-7}$}& & {$6.26$} & &
\\
 GA+TPS - enhanced HRKs scheme ($\rho=10^{-8}$, n=8) & & {} & & {} & & {} & &{$2.1024 \times 10^{-7}$}& & {$9.42$} & &
\\
 MQ+TPS - enhanced HRKs scheme ($\rho=10^{-8}$, n=8) & & {} & & {} & & {} & &{$5.6021 \times 10^{-7}$}& & {$8.17$} & & \\
\bottomrule 
\end{tabular}}  
\label{ab1}
\end{table}  





\end{example}




\newpage
\vspace*{5cm}
\newpage


\newpage
\vspace*{5cm}
\newpage
\begin{example} \label{az70} 

Consider the following second kind 2D-VIE \cite{a81}
\begin{align} \label{asq1}
u(x,t)=f(x,t)+(x+t-z-y) \int_{0}^{t} \int_{0}^{x} u^2(y,z) dy dz, \quad (x,y) \in [0,1] \times [0,1],
\end{align}
in which
\[ f(x,y)= x+t-\frac{1}{12}xt(x^3+4x^2t+4xt^2+t^3).
\]
The true solution is $u(x,t)= x+t.$ In this example, to approximate integrals, 5-point CGL integration scheme with $P=7$ has been utilized.
The comparison of obtained errors by the offered hybrid approach and the rationalized Haar functions (RHFs) method \cite{a81} are given in Table \ref{a550}.
The numerical experiments have been carried out on the chosen grid points which are proposed as $(x,t)=\Bigl ( \frac{1}{2^l}$ , $\frac{1}{2^l} \Bigr ), l=1, 2, \cdots, 6.$
This table illustrates that the new approach achieves greater accuracy than the RHFs method while utilizing fewer collocation nodes.
Additionally, Figure \ref{F14} illustrates the convergence of RMS error for different kernels across various values of $n$.
 Since the MQ and GA kernels possess smoothness of high order, one might anticipate rapid convergence. Nevertheless, our experiments indicate that fixing the shape parameter $\varepsilon$ causes the discretized system to suffer from increasing ill-conditioning as $n$ grows. Consequently, for large $n$, accuracy does not improve significantly and the observed convergence rate deteriorates.
By contrast, the TPS and CU kernels does not suffer this ill-conditioning due to their stability properties.
However, the limited convergence rate is provided by the usage of these kernels. Additionally, hybrid kernels deliver greater accuracy compared to pure kernels.
Figure \ref{F14} further shows that with increasing $n$, the GA+CU hybrid kernel achieves better performance than alternatives like MQ+CU, GA+TPS, and MQ+TPS. Overall, the experiments demonstrate that mixing piecewise smooth radial kernels with infinitely smooth ones yields notable gains in accuracy.
Furthermore, Figure \ref{F32} provides a graphical illustration of the absolute error for different kernels when $n=60$.
In Figures \ref{F14} and \ref{F32}, the parameters were set to  $\varepsilon=0.2$ and $\rho=10^{-10}$. 





\begin{table}[!htb] 
\caption{\small Comparison of the proposed hybrid method and RHFs method  \cite{a81} for example  \ref{az70}.} \vspace*{0.1cm} \scalebox{0.7}{
\begin{tabular}{lcccccccccccc} \toprule 
(x,t)= $\Bigl ( \frac{1}{2^l}$ , $\frac{1}{2^l} \Bigr )$ & & &  RHFs method \cite{a81} & & & Presented method $(n=60)$& &  & & & & \\ \cmidrule(lr){3-6}   \cmidrule(lr){7-11} 
& & {$\alpha=3$} & & {$\alpha=6$} & & {GA+CU} & & {MQ+CU}& & & &  \\
& & {$(n=256)$} & & {$(n=1024)$} & & $( \varepsilon_{opt}=0.42, \rho_{opt}=4.7 \times10^{-10})$ & & $( \varepsilon_{opt}=0.76, \rho_{opt}=7.8 \times10^{-9})$& & & &
\\ \cmidrule(lr){1-11} 
$l=1$ & & {$6.2 \times 10^{-2}$} & & {$3.1 \times 10^{-2} $} & & {$2.4 \times 10^{-13} $} & &{$3.8 \times 10^{-12} $}& & & &
\\
$l=2$ & & {$6.2 \times 10^{-2}$} & & {$3.1 \times 10^{-2}$} & & {$5.9 \times 10^{-9} $} & &{$1.5 \times 10^{-8} $}& & & &
\\
$l=3$ & & {$6.2 \times 10^{-2}$} & & {$3.1 \times 10^{-2} $} & & {$8.2 \times 10^{-8} $} & &{$6.1 \times 10^{-9} $}& & & &
\\
$l=4$ & & {$6.2 \times 10^{-2}$} & & {$3.1 \times 10^{-2}$} & & {$2.2 \times 10^{-10} $}& &{$4.2 \times 10^{-8} $}& & & &
\\ 
 $l=5$ & & {$6.9 \times 10^{-8}$} & & {$3.1 \times 10^{-2}$} & & {$1.1 \times 10^{-8} $} & &{$1.7 \times 10^{-8} $}& & & &
\\ 
 $l=6$ & & {$3.1 \times 10^{-2}$} & & {$2.2 \times 10^{-9}$} & & {$3.4 \times 10^{-8} $} & &{$3.9 \times 10^{-8} $}& & & &
\\ 
  $L_{\infty}$-error & & {$1.72 \times 10^{-3}$} & & {$4.58 \times 10^{-4}$} & & {$3.4 \times 10^{-7} $}& &{$9.1 \times 10^{-7} $}& & & &
\\ 
\bottomrule
\end{tabular}}
\label{a550}
\end{table}






\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.9\textwidth]{2AA.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{RMS error convergence for different  kernels.}
\label{F14} % Give a unique label
\end{figure}




\begin{figure}[!htb]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
\begin{center}
\includegraphics[width=0.7\textwidth]{2BB.png}
% figure caption is below the figure
\end{center}
\vspace*{-0.3cm} \caption{Absolute error in example \ref{az70} for  various kernels with $n=60$. }
\label{F32} % Give a unique label
\end{figure}






\end{example}


\vspace*{5cm}
\newpage

\section{Conclusions and future directions} \label{sec7}
This work introduces a stable mesh-free method based on hybrid radial kernels to approximate solutions of nonlinear VIEs of the second kind. The proposed technique applies HRKs to scattered nodes within a collocation scheme, with integrals approximated via the CGL integration rule. The combination of different kernel types improves both stability and adaptability, enabling efficient computation even for small shape parameters and higher degrees of freedom. By transforming the VIE into a system of nonlinear algebraic equations, the method is supported by a convergence analysis. The PSO algorithm is utilized to optimize the shape parameter and weight coefficient within the hybrid kernels. The numerical results highlight the superior performance of the hybrid kernel approach over pure kernel methods, particularly as the number of nodes increases. Incorporating a small proportion of piecewise smooth radial kernels into infinitely smooth radial kernels markedly decreases errors, thereby improving accuracy. Additionally, the HRK-based method adapts seamlessly to various geometries, because of its independence from background meshes or cell structures, making it a versatile tool for solving VIEs in complex domains. While the offered approach mitigates the ill-conditioning challenges inherent in standard kernel approaches, it remains susceptible to potential ill-conditioning due to its global nature. Although hybrid kernels reduce sensitivity to the shape parameter, they do not completely eliminate it. Future research should aim to improve the stability of the method, potentially through the integration of local approximation techniques or the development of adaptive parameter selection strategies. Extending the approach to higher-dimensional problems and investigating its applicability to other classes of integral equations represent promising directions for further exploration.

\textbf{Conflict of interest} The authors declare that they have no competing interests.


\begin{thebibliography}{00}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem{a00}
 P. Assari, F. Asadi-Mehregan, M. Dehghan, On the numerical solution of Fredholm integral equations
utilizing the local radial basis function method, Int. J. Comput. Math. 96 (2019) 1416–1443.


\bibitem{a11}
C. T. Sheng, Z. Q. Wang, B. Y. Guo, A multistep Legendre–Gauss spectral collocation method for
nonlinear Volterra integral equations, SIAM J. Numer. Anal. 52 (2014) 1953–1980.

{\color{red}
\bibitem{aa12}
M. Heydari,   E. Shivanian,  B. Azarnavid, S. Abbasbandy, An iterative multistep kernel based method for nonlinear Volterra integral and integro-differential equations of fractional order, J. Comput. Appl. Math. 361 (2019) 97–112.


\bibitem{bb1}
A. M. Rismani, H. Monfared, Numerical solution of singular IVPs of Lane–Emden type using a
modified Legendre-spectral method, Appl. Math. Model. 36 (2012) 4830–4836.
 
}

\bibitem{a13}
C. T. Sheng, Z. Q. Wang,  B. Y. Guo, An hp-spectral collocation method for nonlinear Volterra functional integro-differential equations with delays, Appl. Numer. Math. 105 (2016) 1–24.

\bibitem{a14}
Y. Yang,  Y. Chen, Jacobi spectral Galerkin and iterated methods for nonlinear Volterra integral equation, J. Comput. Nonlinear Dyn. 11 (4) (2016) 041027.

\bibitem{a15}
 P. Assari, M. Dehghan, A meshless local discrete Galerkin (MLDG) scheme for numerically
solving two-dimensional nonlinear Volterra integral equations, Appl. Math. Comput. 350 (2019)
249–265.


%\bibitem{a16}
%K. Maleknejad J. Rashidinia, T. Eftekhari, Numerical solution of three-dimensional Volterra–Fredholm integral equations of the first and second kinds based on Bernstein’s approximation, Appl. Math. Comput. 339 (2018) 272–285.

\bibitem{a17}
K. Maleknejad,  R. Dehbozorgi, Adaptive numerical approach based upon Chebyshev operational vector for nonlinear Volterra integral equations and its convergence analysis, J. Comput. Appl. Math. 344 (2018) 356–366.

\bibitem{a22}
S. Sohrabi, H. Ranjbar,  M. Saei, Convergence analysis of the Jacobi-collocation method for nonlinear weakly singular Volterra integral equations, Appl. Math. Comput. 299 (2017) 141–152.


\bibitem{a18}
H. Saeedi,  M. M. Moghadam, Numerical solution of nonlinear Volterra integro-differential equations of arbitrary order by CAS wavelets, Commun. Nonlinear Sci. Numer. Simul. 16 (3) (2011) 1216–1226.

\bibitem{a19}
 I. Aziz, S.-u. Islam, New algorithms for the numerical solution of nonlinear Fredholm and Volterra
integral equations using Haar wavelets, J. Comput. Appl. Math. 239 (2013) 333–345.


\bibitem{a20}
 S.-u. Islam, I. Aziz, A. S. Al-Fhaid, An improved method based on Haar wavelets for numerical
solution of nonlinear integral and integro-differential equations of first and higher orders,
J. Comput. Appl. Math. 260 (2014) 449–469.



\bibitem{a21}
 I. Aziz, S.-u. Islam, F. Khan, A new method based on Haar wavelet for the numerical solution of
two-dimensional nonlinear integral equations, J. Comput. Appl. Math. 272 (2014) 70–80.




\bibitem{a23}
E. Babolian, S. Javadi,  E. Moradi, Error analysis of reproducing kernel Hilbert space method for solving functional integral equations, J. Comput. Appl. Math. 300 (2016) 300–311.

\bibitem{a24}
V. Sizikov, D. Sidorov, Generalized quadrature for solving singular integral equations of Abel type in application to infrared tomography, Appl. Numer. Math. 106 (2016) 69–78.

\bibitem{a25}
M. I. Berenguer,  D. Gamez, A computational method for solving a class of two dimensional Volterra integral equations, J. Comput. Appl. Math. 318 (2017) 403–410.

%\bibitem{a26}
%H. Guoqiang, W. Jiong, Richardson extrapolation of iterated discrete Galerkin solution for two-dimensional Fredholm integral equations, J. Comput. Appl. Math., 139 (2002) 49-63.
%
%\bibitem{a27}
%I. G. Graham. Collocation methods for two dimensional weakly singular integral equations. J. Austral. Math. Soc. (Series B), 22 (1993) 456–473.
%
%\bibitem{a28}
%V. Carutasu. Numerical solution of two-dimensional nonlinear Fredholm integral equations of the second kind by spline functions. General. Math., 9 (2001) 31–48.
%
%\bibitem{a29}
%H. Guoqiang and W. Jiong. Extrapolation of Nystrom solution for two dimensional nonlinear Fredholm integral equations. J. Comput. Appl. Math., 134(1-2) (2001) 259–268.
%
%\bibitem{a30}
%E. Babolian, S. Bazm and P. Lima. Numerical solution of nonlinear two-dimensional integral equations using rationalized Haar functions. Commun. Nonlinear. Sci. Numer. Simulat. 16(3) (2011) 1164–1175.
%
%\bibitem{a31}
%A. Tari, M. Y. Rahimi, S. Shahmorad and F. Talati. Solving a class of two-dimensional linear and nonlinear Volterra integral equations by the differential transform method. J. Comput. Appl. Math. 228(1) (2009) 70–76.

\bibitem{a32}
 R. L. Hardy, Multiquadric equations of topography and other irregular surfaces,
Geophys. Res. 76 (8) (1971) 1905–1915.


\bibitem{a33}
 J. Meinguet, Multivariate interpolation at arbitrary points made simple,
Appl. Math. Phys. (ZAMP) 30 (1979) 292–304.



\bibitem{a42}
 B. Shaback, Meshfree methods for complicated domains and moving boundaries in solid mechanics,
Eng. Anal. Bound. Elem. 32 (2008) 533–553.


\bibitem{a38}
D. Stevens,  H. Power, RBF-FD for Shallow Water Equations, J. Comput. Phys. 342 (2017) 86–105.

\bibitem{a39}
B. Sarler, G. Kuhn, RBF Solution of Stefan Problems, Eng. Anal. Bound. Elem. 87 (2018) 1-12.



\bibitem{a46}
 S. N. Atluri, S. Shen, The meshless local Petrov–Galerkin (MLPG) method,
CMES-Comput. Model. Eng. Sci. 3 (2002) 11–52.



{\color{red}
\bibitem{assari1}
P. Assari, M. Dehghan, The approximate solution of nonlinear Volterra integral equations of the second kind using radial basis functions, Appl. Numer. Math. 131 (2018) 140-157.





\bibitem{assari2}
P. Assari, M. Dehghan, A meshless local Galerkin method for solving Volterra integral equations deduced from nonlinear fractional differential equations using the moving least squares technique, Appl. Numer. Math. 143 (2019) 276-299.


}

%\bibitem{a34}
%G. Wahba, Spline models for observational data, (1990).
%
%\bibitem{a35}
%R. Franke, Scattered Data Interpolation: Tests of Some Methods, Mathematics of Computation, 38(157) (1982) 181-200.
%
%\bibitem{a36}
%E. J. Kansa, A scattered data approximation scheme with applications to computational fluid-dynamics II, Comput. Appl. Math., 19(8/9) (1990) 147-161.
%
%\bibitem{a37}
%M. J. Berger, C. Helzel, R. J. LeVeque, Stable Radial Basis-Finite Difference (RBF-FD) methods for shock wave propagation, J. Comput. Phys., 230(14) (2011) 5359-5383.
%
%
%
%\bibitem{a040}
%Z. Bai, H. Lü, Positive solutions for boundary value problems of nonlinear fractional differential equations, J. Math. Anal. Appl., 311(2) (2005) 495-505.

%\bibitem{a041}
%Y. Zhang, A. B.  Kennedy, A new Boussinesq method for fully nonlinear waves from shallow to deep water, J. Fluid Mech., 826 (2017) 664-693.

%\bibitem{a40}
%G. R. Liu, Y. T. Gu, An Introduction to Meshfree Methods and Their Programming., (2005).
%
%\bibitem{a41}
%E. J. Kansa, A scattered data approximation scheme with applications to computational fluid-dynamics I, Comput. Appl. Math. 25 (1990) 127-145.
%
%
%
%\bibitem{a43}
%Belytschko, Ted, Y. Y. Lu, Gu, Lei, Element-free Galerkin methods, International Journal for Numerical Methods in Engineering, 37 (2) (1994) 229-256.
%
%\bibitem{a44}
%J. S. Chen, M. Hillman, M. Rüter, Reproducing Kernel Particle Method for three-dimensional metal forming simulations, International Journal of Plasticity, 99 (2017) 1-32.





{\color{red}
\bibitem{a47}
M. Heidari, M. Mohammadi,  S. De Marchi, Curvature based characterization of radial basis functions: application to interpolation, Math. Model. Anal. 28 (3) (2023) 415–433.


\bibitem{a48}
F. N. Mojarrad, M. H. Veiga, J. S. Hesthaven,  A new variable shape parameter strategy for RBF approximation using neural networks, Comput. Math. Appl. 143 (2023) 151–168.


\bibitem{a49}
R. Cavoretto, S. De Rossi,  S. Lancellotti, Bayesian approach for radial kernel parameter tuning, J. Comput. Appl. Math. 441 (2024) 115716.
}
\bibitem{a50}
G. Fasshauer,  M. McCourt, Kernel-Based Approximation  {\color{red}Methods } using MATLAB, World Scientific, Interdisciplinary Mathematical Sciences, 2015.

{\color{red}
\bibitem{a480}
R. Cavoretto, G. E. Fasshauer, M. McCourt,  An introduction to the Hilbert‑Schmidt SVD using iterated Brownian bridge kernels, Numer. Algor. 68 (2) (2015) 393–422.



\bibitem{a481}
 G. E. Fasshauer, M. J. McCourt,  Stable evaluation of Gaussian radial basis function interpolants, SIAM J. Sci. Comput. 34 (2) (2012) A737–A762.
}


\bibitem{a51}
P. K. Mishra, S. K. Nath, G. Kosec,  G. E. Fasshauer, Hybrid Gaussian-cubic radial basis functions for scattered data interpolation, Comput. Geosci. 22 (5) (2018) 1203–1218.

%\bibitem{a52}
%P. K. Mishra, G. E. Fasshauer, M. K. Sen, L. Ling, A stabilized radial basis-finite difference (RBF-FD) method with hybrid kernels, Comput. Math. Appl. 77 (9) (2019) 2354–2368.

\bibitem{a53}
Y. Yang, X. Qingyu, L. Qiude, W. Chao, G. Min,  W. Kai, A hybrid kernel function approach for acoustic reconstruction of temperature distribution, Measurement 166 (2020) 108238.

\bibitem{a54}
O. Ömer, A local hybrid kernel meshless method for numerical solutions of two-dimensional fractional cable equation in neuronal dynamics, Numer. Methods Partial Differ. Equ. 36 (6) (2020) 1699–1717.

\bibitem{a55}
H. Manzoor, Hybrid radial basis function methods of lines for the numerical solution of viscous Burgers’ equation, Comput. Appl. Math. 40 (2021) 1–49.

\bibitem{a56}
T. Akbari, M. Esmaeilbeigi,  D. Moazami, A stable meshless numerical scheme using hybrid kernels to solve linear Fredholm integral equations of the second kind and its applications, Math. Comput. Simul. 220 (2024) 1–28.

\bibitem{44}
H. Wendland, Scattered data approximation, Cambridge University Press, 2005.

{\color{red}
\bibitem{a700}
M. Hussain, A. Ghafoor, A. Hussain, S. Haq, I. Ali, S. U. Arifeen,
A hybrid kernel-based meshless method for numerical approximation of multidimensional Fisher’s equation,
Math. Comput. Simul. 223 (2024) 130–157.
}


\bibitem{31}
G. E. Fasshauer, Meshfree approximation methods with MATLAB, World Scientific Publishing Co., Inc., River Edge, NJ,
USA, 2007.

\bibitem{37}
R. Cavoretto, A. De Rossi, M. S. Mukhametzhanov,  Ya. D. Sergeyev, On the search of the shape parameter in radial basis functions using univariate global optimization methods, J. Glob. Optim. 79 (2019) 305–327.

\bibitem{39}
M. Hussain, S. Haq, A computational study of solitary waves solution of Kawahara-type equations by meshless spectral interpolation method, Int. J. Mod. Phys. C. 30 (12) (2019) 1950102.

\bibitem{40}
M. Hussain,  S. Haq, Numerical simulation of solitary waves of Rosenau–KdV equation by Crank–Nicolson meshless spectral interpolation method, Eur. Phys. J. Plus. 98  (2020).

\bibitem{42}
S. A. Sarra,  E. J. Kansa, Multiquadric radial basis function approximation methods for the numerical solution of partial differential equations, Adv. Comput. Mech. 2 (2009) 1940-5820.

\bibitem{43}
P. K. Mishra, G. E. Fasshauer, M. K. Sen,  L. Ling, A stabilized radial basis-finite difference (RBF-FD) method with hybrid kernels, Comput. Math. Appl. 77(9) (2019) 2354–2368.

%%%%%%%%%%%%47 ta

%
%\bibitem{a58}
%E. Hairer, C. Lubich, M. Schlichte, Fast numerical solution of weakly singular Volterra integral equations, J. Comput. Appl. Math. 23 (1) (1988) 87–98.
%
%\bibitem{a59}
%F. G. Friedlander, The reflexion of sound pulses by convex parabolic reflectors, Math. Proc. Cambridge Philos. Soc. 37 (2) (1941) Cambridge University Press.
%
%\bibitem{a60}
%A. M. Rismani, H. Monfared, Numerical solution of singular IVPs of Lane–Emden type using a modified Legendre-spectral method, Appl. Math. Model. 36 (10) (2012) 4830–4836.
%
%\bibitem{a61}
%U. M. Schaudt, On static stars in Newtonian gravity and Lane–Emden type equations, Ann. Henri Poincaré 1 (5) (2000) Birkhäuser Verlag.
%
%\bibitem{a62}
%B. Azarnavid, F. Parvaneh, S. Abbasbandy, Picard-reproducing kernel Hilbert space method for solving generalized singular nonlinear Lane–Emden type equations, Math. Model. Anal. 20 (6) (2015) 754–767.
%
%\bibitem{a63}
%G. Gripenberg, S.O. Londen, O. Staffans, Volterra Integral and Functional Equations. Vol. 34, Cambridge University Press, 1990.

\bibitem{a64}
 S. Zhang, Y. Lin, M. Rao, Numerical solutions for second-kind Volterra integral equations by
Galerkin methods, Appl. Math. 45 (2000) 19–39.


\bibitem{a65}
M. D. Buhmann, Radial Basis Functions: Theory and Implementations, Cambridge University Press, Cambridge, 2003.

%%%%%%%%%%% 50 ta
%\bibitem{a66}
%G. E. Fasshauer. Meshfree methods. In Handbook of Theoretical and Computational Nanotechnology. American Scientific Publishers, 2005.

\bibitem{a67}
 W. Fang, Y. Wang, Y. Xu, An implementation of fast wavelet Galerkin methods for integral
equations of the second kind, J. Sci. Comput. 20 (2004) 277–302.


%\bibitem{a68}
%H. Kaneko and Y. Xu. Gauss-type quadratures for weakly singular integrals and their application to Fredholm integral equations of the second kind. Math. Comp., 62(206) (1994) 739–753.
%
%\bibitem{a69}
%S. Mirjalili, A. Lewis. The whale optimization algorithm. Adv. Eng. Softw., 95 (2016) 51-67.
%
%\bibitem{a70}
%N. Khodadadi, A. Ö. Çiftçioğlu, S. Mirjalili, A. Nanni. A comparison performance analysis of eight meta-heuristic algorithms for optimal design of truss structures with static constraints. Decis. Analytics. J., 8 (2023) 100266.
%
%\bibitem{a71}
%S. Khalilpourazari, S. Khalilpourazary. An efficient hybrid algorithm based on water cycle and Moth–Flame optimization algorithms for solving numerical and constrained engineering optimization problems. Soft. Comput., 23 (5) (2019) 1699-1722.

%\bibitem{a72}
%H. Faris, S. Mirjalili, I. Aljarah, M. Mafarja, A. A. Heidari. Salp swarm algorithm: Theory, literature review, and application in extreme learning machines. Stud. Comput. Intell. 811 (2020) 185-199.

%\bibitem{a73}
%X. S. Yang, S. Deb, S. Fong, X. He, Y. X. Zhao. From swarm intelligence to metaheuristics: Natureinspired optimization algorithms. Computer. 49 (9) (2016) 52-59.

%\bibitem{a74}
%J. Kennedy, R. Eberhart. Particle swarm optimization, in: Proceedings of IEEE international
%conference on neural networks. IEEE., (1995) 1942-1948.
%
%\bibitem{a75}
%D. Tian, X. Zhao, Z. Shi. Chaotic particle swarm optimization with sigmoid-based acceleration coefficients for numerical function optimization. Swarm. Evol. Comput.    51 (2019) 100573. 
%
%
%\bibitem{a76}
%K. Chen, F. Zhou, L. Yin, S. Wang, Y. Wang, F. Wan. A hybrid particle swarm optimizer with sine cosine acceleration
%coefficients. Inf. Sci. 422 (2018) 218-241. 


{\color{red}
\bibitem{Rippa} 
S. Rippa, An algorithm for selecting a good value for the parameter c in radial basis function interpolation, Adv. Comput. Math. 11(2-3) (1990) 193–210.
}
\bibitem{a77}
D. Moazami,  M. Esmaeilbeigi, Enhanced stability and accuracy in solving nonlinear Fredholm integral equations using hybrid radial kernels and particle swarm optimization, Comput. Appl. Math. 44 (78) (2025).


\bibitem{a78}
B. Azarnavid, F. Parvaneh,  S. Abbasbandy, Picard-reproducing kernel Hilbert space method for solving generalized singular nonlinear Lane–Emden type equations, Math. Model. Anal. 20 (6) (2015) 754–767. 




\bibitem{a81}
E. Babolian, S. Bazm,  P. Lima, Numerical solution of nonlinear two-dimensional integral equations using rationalized Haar functions, Commun. Nonlinear Sci. Numer. Simul. 16 (2011) 1164–1175. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{thebibliography}

\end{document}
